{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "treated-drama",
   "metadata": {},
   "source": [
    "# The Annotated GAT (PPI)\n",
    "\n",
    "This notebook is the 2nd part of the series, please check out **\"The Annotated GAT (Cora)\"** notebook for a more gentle introduction to GAT. ‚ù§Ô∏è\n",
    "\n",
    "The idea of this notebook is to explain how you can use GAT in an **inductive setting**. \n",
    "\n",
    "I'll be using the **PPI (protein-protein interaction) dataset** in this notebook.\n",
    "\n",
    "Here is a representation of the 3D structure of the protein [myoglobin](https://en.wikipedia.org/wiki/Protein) (not like you need to know anything about proteins in order to follow along this notebook it's just that they are beautiful and I love them üçó‚ù§Ô∏è)!\n",
    "\n",
    "<img src=\"data/readme_pics/protein.png\" alt=\"protein schematic\" align=\"center\"/> <br/>\n",
    "\n",
    "In this notebook you'll get the answers to these questions:\n",
    "\n",
    "‚úÖ How to load and visualize the PPI dataset? <br/>\n",
    "‚úÖ How to train/use GAT on PPI (multi-label classification problem)? <br/>\n",
    "‚úÖ How to visualize different GAT's properties? (mainly attention) <br/>\n",
    "\n",
    "Awesome, let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "described-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I always like to structure my imports into Python's native libs,\n",
    "# stuff I installed via conda/pip and local file imports (but we don't have those here)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import enum\n",
    "\n",
    "# Visualization related imports\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import igraph as ig\n",
    "\n",
    "# Main computation libraries\n",
    "import numpy as np\n",
    "\n",
    "# Deep learning related imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.hub import download_url_to_file\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "modified-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Contains constants needed for data loading and visualization.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Supported datasets - only PPI in this notebook\n",
    "class DatasetType(enum.Enum):\n",
    "    PPI = 0\n",
    "\n",
    "    \n",
    "class GraphVisualizationTool(enum.Enum):\n",
    "    IGRAPH = 0\n",
    "\n",
    "\n",
    "# We'll be dumping and reading the data from this directory\n",
    "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data')\n",
    "PPI_PATH = os.path.join(DATA_DIR_PATH, 'ppi')\n",
    "PPI_URL = 'https://data.dgl.ai/dataset/ppi.zip'  # preprocessed PPI data from Deep Graph Library\n",
    "\n",
    "#\n",
    "# PPI specific constants\n",
    "#\n",
    "\n",
    "PPI_NUM_INPUT_FEATURES = 50\n",
    "PPI_NUM_CLASSES = 121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-failing",
   "metadata": {},
   "source": [
    "Note that some parts of this notebook overlap with \"The Annotated GAT (Cora)\" notebook, so you may see some redundancy. üôè\n",
    "\n",
    "With that out of the way we've got the level 1 unlocked (Data üìú). üòç Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-mortality",
   "metadata": {},
   "source": [
    "# Part 1: Understanding your data (become One with the data üìú‚ù§Ô∏è)\n",
    "\n",
    "I'll be using the PPI dataset as the running example in this notebook.\n",
    "\n",
    "Having said that, you may wonder, what's the difference between `transductive` and `inductive` setting? If you're not familiar with GNNs this may appear as a weird concept. But it's quite simple actually.\n",
    "\n",
    "**Transductive** - you have a single graph (like Cora) you split some **nodes** (and not graphs) into train/val/test training sets. While you're training you'll be using only the labels from your training nodes. BUT. During the forward prop, by the nature of how spatial GNNs work, you'll be aggregating the feature vectors from your neighbors and **some of them may belong to val or even test sets!** The main point is - you **ARE NOT** using their label information but you **ARE** using the structural information and their features.\n",
    "\n",
    "**Inductive** - you're probably much more familiar with this one if you come from the computer vision or NLP background. You have a set of training graphs, a separate set of val graphs and of course a separate set of test graphs.\n",
    "\n",
    "Having explained that let's jump into the code and let's load and visualize PPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "variable-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's define this simple function for loading PPI's graph data\n",
    "\n",
    "def json_read(path):\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-child",
   "metadata": {},
   "source": [
    "Now let's see how we can load PPI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "metropolitan-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_data(training_config, device):\n",
    "    dataset_name = training_config['dataset_name'].lower()\n",
    "    should_visualize = training_config['should_visualize']\n",
    "\n",
    "    if dataset_name == DatasetType.PPI.name.lower():  # Protein-Protein Interaction dataset\n",
    "\n",
    "        # Instead of checking PPI in, I'd rather download it on-the-fly the first time it's needed (lazy execution ^^)\n",
    "        if not os.path.exists(PPI_PATH):  # download the first time this is ran\n",
    "            os.makedirs(PPI_PATH)\n",
    "\n",
    "            # Step 1: Download the ppi.zip (contains the PPI dataset)\n",
    "            zip_tmp_path = os.path.join(PPI_PATH, 'ppi.zip')\n",
    "            download_url_to_file(PPI_URL, zip_tmp_path)\n",
    "\n",
    "            # Step 2: Unzip it\n",
    "            with zipfile.ZipFile(zip_tmp_path) as zf:\n",
    "                zf.extractall(path=PPI_PATH)\n",
    "            print(f'Unzipping to: {PPI_PATH} finished.')\n",
    "\n",
    "            # Step3: Remove the temporary resource file\n",
    "            os.remove(zip_tmp_path)\n",
    "            print(f'Removing tmp file {zip_tmp_path}.')\n",
    "\n",
    "        # Collect train/val/test graphs here\n",
    "        edge_index_list = []\n",
    "        node_features_list = []\n",
    "        node_labels_list = []\n",
    "\n",
    "        # Dynamically determine how many graphs we have per split (avoid using constants when possible)\n",
    "        num_graphs_per_split_cumulative = [0]\n",
    "\n",
    "        # Small optimization \"trick\" since we only need test in the playground.py\n",
    "        splits = ['test'] if training_config['ppi_load_test_only'] else ['train', 'valid', 'test']\n",
    "\n",
    "        for split in splits:\n",
    "            # PPI has 50 features per node, it's a combination of positional gene sets, motif gene sets,\n",
    "            # and immunological signatures - you can treat it as a black box (I personally have a rough understanding)\n",
    "            # shape = (NS, 50) - where NS is the number of (N)odes in the training/val/test (S)plit\n",
    "            # Note: node features are already preprocessed\n",
    "            node_features = np.load(os.path.join(PPI_PATH, f'{split}_feats.npy'))\n",
    "\n",
    "            # PPI has 121 labels and each node can have multiple labels associated (gene ontology stuff)\n",
    "            # SHAPE = (NS, 121)\n",
    "            node_labels = np.load(os.path.join(PPI_PATH, f'{split}_labels.npy'))\n",
    "\n",
    "            # Graph topology stored in a special nodes-links NetworkX format\n",
    "            nodes_links_dict = json_read(os.path.join(PPI_PATH, f'{split}_graph.json'))\n",
    "            # PPI contains undirected graphs with self edges - 20 train graphs, 2 validation graphs and 2 test graphs\n",
    "            # The reason I use a NetworkX's directed graph is because we need to explicitly model both directions\n",
    "            # because of the edge index and the way GAT implementation #3 works\n",
    "            collection_of_graphs = nx.DiGraph(json_graph.node_link_graph(nodes_links_dict))\n",
    "            # For each node in the above collection, ids specify to which graph the node belongs to\n",
    "            graph_ids = np.load(os.path.join(PPI_PATH, F'{split}_graph_id.npy'))\n",
    "            num_graphs_per_split_cumulative.append(num_graphs_per_split_cumulative[-1] + len(np.unique(graph_ids)))\n",
    "            # Split the collection of graphs into separate PPI graphs\n",
    "            for graph_id in range(np.min(graph_ids), np.max(graph_ids) + 1):\n",
    "                mask = graph_ids == graph_id  # find the nodes which belong to the current graph (identified via id)\n",
    "                graph_node_ids = np.asarray(mask).nonzero()[0]\n",
    "                graph = collection_of_graphs.subgraph(graph_node_ids)  # returns the induced subgraph over these nodes\n",
    "                print(f'Loading {split} graph {graph_id} to CPU. '\n",
    "                      f'It has {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.')\n",
    "\n",
    "                # shape = (2, E) - where E is the number of edges in the graph\n",
    "                # Note: leaving the tensors on CPU I'll load them to GPU in the training loop on-the-fly as VRAM\n",
    "                # is a scarcer resource than CPU's RAM and the whole PPI dataset can't fit during the training.\n",
    "                edge_index = torch.tensor(list(graph.edges), dtype=torch.long).transpose(0, 1).contiguous()\n",
    "                edge_index = edge_index - edge_index.min()  # bring the edges to [0, num_of_nodes] range\n",
    "                edge_index_list.append(edge_index)\n",
    "                # shape = (N, 50) - where N is the number of nodes in the graph\n",
    "                node_features_list.append(torch.tensor(node_features[mask], dtype=torch.float))\n",
    "                # shape = (N, 121), BCEWithLogitsLoss doesn't require long/int64 so saving some memory by using float32\n",
    "                node_labels_list.append(torch.tensor(node_labels[mask], dtype=torch.float))\n",
    "\n",
    "                if should_visualize:\n",
    "                    plot_in_out_degree_distributions(edge_index.numpy(), graph.number_of_nodes(), dataset_name)\n",
    "                    visualize_graph(edge_index.numpy(), node_labels[mask], dataset_name)\n",
    "\n",
    "        #\n",
    "        # Prepare graph data loaders\n",
    "        #\n",
    "\n",
    "        # Optimization, do a shortcut in case we only need the test data loader\n",
    "        if training_config['ppi_load_test_only']:\n",
    "            data_loader_test = GraphDataLoader(\n",
    "                node_features_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n",
    "                node_labels_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n",
    "                edge_index_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n",
    "                batch_size=training_config['batch_size'],\n",
    "                shuffle=False\n",
    "            )\n",
    "            return data_loader_test\n",
    "        else:\n",
    "\n",
    "            data_loader_train = GraphDataLoader(\n",
    "                node_features_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n",
    "                node_labels_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n",
    "                edge_index_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n",
    "                batch_size=training_config['batch_size'],\n",
    "                shuffle=True\n",
    "            )\n",
    "\n",
    "            data_loader_val = GraphDataLoader(\n",
    "                node_features_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],\n",
    "                node_labels_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],\n",
    "                edge_index_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],\n",
    "                batch_size=training_config['batch_size'],\n",
    "                shuffle=False  # no need to shuffle the validation and test graphs\n",
    "            )\n",
    "\n",
    "            data_loader_test = GraphDataLoader(\n",
    "                node_features_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],\n",
    "                node_labels_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],\n",
    "                edge_index_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],\n",
    "                batch_size=training_config['batch_size'],\n",
    "                shuffle=False\n",
    "            )\n",
    "\n",
    "            return data_loader_train, data_loader_val, data_loader_test\n",
    "    else:\n",
    "        raise Exception(f'{dataset_name} not yet supported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-watts",
   "metadata": {},
   "source": [
    "Nice, there are is this `GraphDataLoader` object that we still haven't defined. We need it in order to load batches of PPI graphs into GAT.\n",
    "\n",
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dental-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "    When dealing with batches it's always a good idea to inherit from PyTorch's provided classes (Dataset/DataLoader).\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, node_features_list, node_labels_list, edge_index_list, batch_size=1, shuffle=False):\n",
    "        graph_dataset = GraphDataset(node_features_list, node_labels_list, edge_index_list)\n",
    "        # We need to specify a custom collate function, it doesn't work with the default one\n",
    "        super().__init__(graph_dataset, batch_size, shuffle, collate_fn=graph_collate_fn)\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This one just fetches a single graph from the split when GraphDataLoader \"asks\" it\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, node_features_list, node_labels_list, edge_index_list):\n",
    "        self.node_features_list = node_features_list\n",
    "        self.node_labels_list = node_labels_list\n",
    "        self.edge_index_list = edge_index_list\n",
    "\n",
    "    # 2 interface functions that need to be defined are len and getitem so that DataLoader can do it's magic\n",
    "    def __len__(self):\n",
    "        return len(self.edge_index_list)\n",
    "\n",
    "    def __getitem__(self, idx):  # we just fetch a single graph\n",
    "        return self.node_features_list[idx], self.node_labels_list[idx], self.edge_index_list[idx]\n",
    "\n",
    "\n",
    "def graph_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    The main idea here is to take multiple graphs from PPI as defined by the batch size\n",
    "    and merge them into a single graph with multiple connected components.\n",
    "\n",
    "    It's important to adjust the node ids in edge indices such that they form a consecutive range. Otherwise\n",
    "    the scatter functions in the implementation 3 will fail.\n",
    "\n",
    "    :param batch: contains a list of edge_index, node_features, node_labels tuples (as provided by the GraphDataset)\n",
    "    \"\"\"\n",
    "\n",
    "    edge_index_list = []\n",
    "    node_features_list = []\n",
    "    node_labels_list = []\n",
    "    num_nodes_seen = 0\n",
    "\n",
    "    for features_labels_edge_index_tuple in batch:\n",
    "        # Just collect these into separate lists\n",
    "        node_features_list.append(features_labels_edge_index_tuple[0])\n",
    "        node_labels_list.append(features_labels_edge_index_tuple[1])\n",
    "\n",
    "        edge_index = features_labels_edge_index_tuple[2]  # all of the components are in the [0, N] range\n",
    "        edge_index_list.append(edge_index + num_nodes_seen)  # very important! translate the range of this component\n",
    "        num_nodes_seen += len(features_labels_edge_index_tuple[1])  # update the number of nodes we've seen so far\n",
    "\n",
    "    # Merge the PPI graphs into a single graph with multiple connected components\n",
    "    node_features = torch.cat(node_features_list, 0)\n",
    "    node_labels = torch.cat(node_labels_list, 0)\n",
    "    edge_index = torch.cat(edge_index_list, 1)\n",
    "\n",
    "    return node_features, node_labels, edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-mechanics",
   "metadata": {},
   "source": [
    "The idea is simple üí° (as all things should be üòú). In order to pass a batch of graphs into GAT we do the following:\n",
    "\n",
    "1. During the preprocessing step, we map the edge index from the original range into the [0, N] range, where N is the number of nodes in a given graph. Example: the third graph in the training split may contain nodes [3500, ..., 5000] and thus originally its edge index would be in that very same range, e.g.: [3500, 4232], [3808, 4232], ...]. By subtracting the min element, 3500 in this case, we bring the edge index into [0, 1500] range.\n",
    "\n",
    "---\n",
    "\n",
    "2. In the `graph_collate_fn` function, edge indices are in the normalized range i.e. [0, N]. What we do is we shift them such that we end up with a single graph that actually consists out of multiple smaller PPI graphs which are not connected with each other i.e. they represent separate [connected components](https://www.geeksforgeeks.org/connected-components-in-an-undirected-graph/). \n",
    "\n",
    "---\n",
    "\n",
    "**Example:** let's say we have 3 graphs in a batch and all 3 edge indices are in the [0, 1000] range. What we'll do is the following: we'll leave the first graph without any modification, we'll shift second graph's range to [1000, 2000], because we had 1000 nodes in the graph that came before, and we'll shift the third graph's range into [2000, 3000], again because we had 2000 nodes that came before this graph. So the final edge index will have nodes in the [0, 3000] range and we treat that as a single graph with multiple connected components. ü§ì\n",
    "\n",
    "Nice, finally let's try and load it. We should also analyze the shapes - that's always a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unexpected-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train graph 1 to CPU. It has 1767 nodes and 34085 edges.\n",
      "Loading train graph 2 to CPU. It has 1377 nodes and 31081 edges.\n",
      "Loading train graph 3 to CPU. It has 2263 nodes and 61907 edges.\n",
      "Loading train graph 4 to CPU. It has 2339 nodes and 67769 edges.\n",
      "Loading train graph 5 to CPU. It has 1578 nodes and 37740 edges.\n",
      "Loading train graph 6 to CPU. It has 1021 nodes and 19237 edges.\n",
      "Loading train graph 7 to CPU. It has 1823 nodes and 46153 edges.\n",
      "Loading train graph 8 to CPU. It has 2488 nodes and 72878 edges.\n",
      "Loading train graph 9 to CPU. It has 591 nodes and 8299 edges.\n",
      "Loading train graph 10 to CPU. It has 3312 nodes and 109510 edges.\n",
      "Loading train graph 11 to CPU. It has 2401 nodes and 66619 edges.\n",
      "Loading train graph 12 to CPU. It has 1878 nodes and 48146 edges.\n",
      "Loading train graph 13 to CPU. It has 1819 nodes and 47587 edges.\n",
      "Loading train graph 14 to CPU. It has 3480 nodes and 110234 edges.\n",
      "Loading train graph 15 to CPU. It has 2794 nodes and 88112 edges.\n",
      "Loading train graph 16 to CPU. It has 2326 nodes and 62188 edges.\n",
      "Loading train graph 17 to CPU. It has 2650 nodes and 79714 edges.\n",
      "Loading train graph 18 to CPU. It has 2815 nodes and 88335 edges.\n",
      "Loading train graph 19 to CPU. It has 3163 nodes and 97321 edges.\n",
      "Loading train graph 20 to CPU. It has 3021 nodes and 94359 edges.\n",
      "Loading valid graph 21 to CPU. It has 3230 nodes and 100676 edges.\n",
      "Loading valid graph 22 to CPU. It has 3284 nodes and 104758 edges.\n",
      "Loading test graph 23 to CPU. It has 3224 nodes and 103872 edges.\n",
      "Loading test graph 24 to CPU. It has 2300 nodes and 63628 edges.\n",
      "********************\n",
      "torch.Size([1021, 50]) torch.float32\n",
      "torch.Size([1021, 121]) torch.float32\n",
      "torch.Size([2, 19237]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Let's just define dummy visualization functions for now - just to stop Python interpreter from complaining!\n",
    "# We'll define them in a moment, properly, I swear.\n",
    "\n",
    "def plot_in_out_degree_distributions():\n",
    "    pass\n",
    "\n",
    "def visualize_graph():\n",
    "    pass\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # checking whether you have a GPU\n",
    "\n",
    "config = {\n",
    "    'dataset_name': DatasetType.PPI.name,\n",
    "    'should_visualize': False,\n",
    "    'batch_size': 1,\n",
    "    'ppi_load_test_only': False  # small optimization for loading test graphs only, we won't use it here\n",
    "}\n",
    "\n",
    "data_loader_train, data_loader_val, data_loader_test = load_graph_data(config, device)\n",
    "# Let's fetch a single batch from the train graph data loader\n",
    "node_features, node_labels, edge_index = next(iter(data_loader_train))\n",
    "\n",
    "print('*' * 20)\n",
    "print(node_features.shape, node_features.dtype)\n",
    "print(node_labels.shape, node_labels.dtype)\n",
    "print(edge_index.shape, edge_index.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-friday",
   "metadata": {},
   "source": [
    "Nice! Analyzing the shapes we see the following:\n",
    "1. This specific PPI train graph (batch size = 1) has 3021 nodes \n",
    "2. Each node has **50 features** (check out [data_loading.py](https://github.com/gordicaleksa/pytorch-GAT/blob/main/utils/data_loading.py) for much more detail)\n",
    "3. PPI has **121 classes** and each node can have multiple classes associated with it (multi-label classification dataset)\n",
    "4. This graph has 94359 edges (including the self edges)! (Compare this to 13k edges in Cora)\n",
    "5. PPI has **20 train** graphs, **2 validation** graphs and **2 test** graphs\n",
    "\n",
    "Additionally the edge index is of `int 64` type. Why? Well it's a constraint that PyTorch is imposing upon us. `index_select` functions require torch.long (i.e. 64 bit integer) and we use those in GAT implementation 3 - that's it.\n",
    "\n",
    "node_labels can be `float32` because `nn.BCEWithLogitsLoss` doesn't require long/int64 type (compare that to `nn.CrossEntropyLoss`, that we used in the Cora notebook, which does require int64s). So we can save up 2x memory! Not bad.\n",
    "\n",
    "---\n",
    "\n",
    "On the \"side note\", it's always a **good idea to test your code as you're progressing with your project.** \n",
    "\n",
    "Data loading is completely orthogonal to the rest of this notebook so we can test it, standalone, and make sure the shapes and datatypes make sense. I use this strategy while developing projects like this one (and in general).\n",
    "\n",
    "I start with data, I add the loading functionality, I add some visualizations and only then do I usually start developing the deep learning model itself.\n",
    "\n",
    "Visualizations are a huge bonus, so let's develop them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-regulation",
   "metadata": {},
   "source": [
    "# Part 2: Understanding GAT's inner workings üíªü¶Ñ\n",
    "\n",
    "First let's create a high level class where we'll build up `GAT` from `GatLayer` objects. \n",
    "\n",
    "It basically just stacks the layers into a nn.Sequential object and additionally since nn.Sequential expects a single input (and it has a single output) I just pack the data (features, edge index) into a tuple - *pure syntactic sugar*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "hourly-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The most interesting and hardest implementation is implementation #3.\n",
    "    Imp1 and imp2 differ in subtle details but are basically the same thing.\n",
    "\n",
    "    So I'll focus on imp #3 in this notebook.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection=True, bias=True,\n",
    "                 dropout=0.6, log_attention_weights=False):\n",
    "        super().__init__()\n",
    "        assert num_of_layers == len(num_heads_per_layer) == len(num_features_per_layer) - 1, f'Enter valid arch params.'\n",
    "\n",
    "        num_heads_per_layer = [1] + num_heads_per_layer  # trick - so that I can nicely create GAT layers below\n",
    "\n",
    "        gat_layers = []  # collect GAT layers\n",
    "        for i in range(num_of_layers):\n",
    "            layer = GATLayer(\n",
    "                num_in_features=num_features_per_layer[i] * num_heads_per_layer[i],  # consequence of concatenation\n",
    "                num_out_features=num_features_per_layer[i+1],\n",
    "                num_of_heads=num_heads_per_layer[i+1],\n",
    "                concat=True if i < num_of_layers - 1 else False,  # last GAT layer does mean avg, the others do concat\n",
    "                activation=nn.ELU() if i < num_of_layers - 1 else None,  # last layer just outputs raw scores\n",
    "                dropout_prob=dropout,\n",
    "                add_skip_connection=add_skip_connection,\n",
    "                bias=bias,\n",
    "                log_attention_weights=log_attention_weights\n",
    "            )\n",
    "            gat_layers.append(layer)\n",
    "\n",
    "        self.gat_net = nn.Sequential(\n",
    "            *gat_layers,\n",
    "        )\n",
    "\n",
    "    # data is just a (in_nodes_features, edge_index) tuple, I had to do it like this because of the nn.Sequential:\n",
    "    # https://discuss.pytorch.org/t/forward-takes-2-positional-arguments-but-3-were-given-for-nn-sqeuential-with-linear-layers/65698\n",
    "    def forward(self, data):\n",
    "        return self.gat_net(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-formula",
   "metadata": {},
   "source": [
    "Now for the fun part let's define the layer. \n",
    "\n",
    "I really don't think that I can explain it any better, using words, than you taking your time to digest the code and the comments.\n",
    "\n",
    "Also make sure to check out [my video on GAT](https://www.youtube.com/watch?v=uFLeKkXWq2c) before you start losing time trying to figure it out \"from scratch\". It's always good to have some theoretical background at your hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "outer-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation #3 was inspired by PyTorch Geometric: https://github.com/rusty1s/pytorch_geometric\n",
    "\n",
    "    But, it's hopefully much more readable! (and of similar performance)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # We'll use these constants in many functions so just extracting them here as member fields\n",
    "    src_nodes_dim = 0  # position of source nodes in edge index\n",
    "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
    "\n",
    "    # These may change in the inductive setting - leaving it like this for now (not future proof)\n",
    "    nodes_dim = 0      # node dimension (axis is maybe a more familiar term nodes_dim is the position of \"N\" in tensor)\n",
    "    head_dim = 1       # attention head dim\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\n",
    "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.num_out_features = num_out_features\n",
    "        self.concat = concat  # whether we should concatenate or average the attention heads\n",
    "        self.add_skip_connection = add_skip_connection\n",
    "\n",
    "        #\n",
    "        # Trainable weights: linear projection matrix (denoted as \"W\" in the paper), attention target/source\n",
    "        # (denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)\n",
    "        #\n",
    "\n",
    "        # You can treat this one matrix as num_of_heads independent W matrices\n",
    "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "\n",
    "        # After we concatenate target node (node i) and source node (node j) we apply the \"additive\" scoring function\n",
    "        # which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same.\n",
    "        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\"\n",
    "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
    "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "\n",
    "        # Bias is definitely not crucial to GAT - feel free to experiment (I pinged the main author, Petar, on this one)\n",
    "        if bias and concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_of_heads * num_out_features))\n",
    "        elif bias and not concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        if add_skip_connection:\n",
    "            self.skip_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "        else:\n",
    "            self.register_parameter('skip_proj', None)\n",
    "\n",
    "        #\n",
    "        # End of trainable weights\n",
    "        #\n",
    "\n",
    "        self.leakyReLU = nn.LeakyReLU(0.2)  # using 0.2 as in the paper, no need to expose every setting\n",
    "        self.activation = activation\n",
    "        # Probably not the nicest design but I use the same module in 3 locations, before/after features projection\n",
    "        # and for attention coefficients. Functionality-wise it's the same as using independent modules.\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.log_attention_weights = log_attention_weights  # whether we should log the attention weights\n",
    "        self.attention_weights = None  # for later visualization purposes, I cache the weights here\n",
    "\n",
    "        self.init_params()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        #\n",
    "        # Step 1: Linear Projection + regularization\n",
    "        #\n",
    "\n",
    "        in_nodes_features, edge_index = data  # unpack data\n",
    "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "        assert edge_index.shape[0] == 2, f'Expected edge index with shape=(2,E) got {edge_index.shape}'\n",
    "\n",
    "        # shape = (N, FIN) where N - number of nodes in the graph, FIN - number of input features per node\n",
    "\n",
    "        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features\n",
    "        # We project the input node features into NH independent output features (one for each attention head)\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "        print(\"features_proj (node 1, head 1) = \", nodes_features_proj[1][1])\n",
    "\n",
    "        #\n",
    "        # Step 2: Edge attention calculation\n",
    "        #\n",
    "\n",
    "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension\n",
    "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
    "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "\n",
    "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
    "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
    "        # by the edge index.\n",
    "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
    "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "        scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted)\n",
    "\n",
    "        # shape = (E, NH, 1)\n",
    "        attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\n",
    "\n",
    "        #\n",
    "        # Step 3: Neighborhood aggregation\n",
    "        #\n",
    "\n",
    "        # Element-wise (aka Hadamard) product. Operator * does the same thing as torch.mul\n",
    "        # shape = (E, NH, FOUT) * (E, NH, 1) -> (E, NH, FOUT), 1 gets broadcast into FOUT\n",
    "        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n",
    "\n",
    "        # This part sums up weighted and projected neighborhood feature vectors for every target node\n",
    "        # shape = (N, NH, FOUT)\n",
    "        out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\n",
    "\n",
    "        #\n",
    "        # Step 4: Residual/skip connections, concat and bias\n",
    "        #\n",
    "\n",
    "        out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)\n",
    "        return (out_nodes_features, edge_index)\n",
    "\n",
    "    #\n",
    "    # Helper functions (without comments there is very little code so don't be scared!)\n",
    "    #\n",
    "\n",
    "    def neighborhood_aware_softmax(self, scores_per_edge, trg_index, num_of_nodes):\n",
    "        \"\"\"\n",
    "        As the fn name suggest it does softmax over the neighborhoods. Example: say we have 5 nodes in a graph.\n",
    "        Two of them 1, 2 are connected to node 3. If we want to calculate the representation for node 3 we should take\n",
    "        into account feature vectors of 1, 2 and 3 itself. Since we have scores for edges 1-3, 2-3 and 3-3\n",
    "        in scores_per_edge variable, this function will calculate attention scores like this: 1-3/(1-3+2-3+3-3)\n",
    "        (where 1-3 is overloaded notation it represents the edge 1-3 and its (exp) score) and similarly for 2-3 and 3-3\n",
    "         i.e. for this neighborhood we don't care about other edge scores that include nodes 4 and 5.\n",
    "\n",
    "        Note:\n",
    "        Subtracting the max value from logits doesn't change the end result but it improves the numerical stability\n",
    "        and it's a fairly common \"trick\" used in pretty much every deep learning framework.\n",
    "        Check out this link for more details:\n",
    "\n",
    "        https://stats.stackexchange.com/questions/338285/how-does-the-subtraction-of-the-logit-maximum-improve-learning\n",
    "\n",
    "        \"\"\"\n",
    "        # Calculate the numerator. Make logits <= 0 so that e^logit <= 1 (this will improve the numerical stability)\n",
    "        scores_per_edge = scores_per_edge - scores_per_edge.max()\n",
    "        exp_scores_per_edge = scores_per_edge.exp()  # softmax\n",
    "\n",
    "        # Calculate the denominator. shape = (E, NH)\n",
    "        neigborhood_aware_denominator = self.sum_edge_scores_neighborhood_aware(exp_scores_per_edge, trg_index, num_of_nodes)\n",
    "\n",
    "        # 1e-16 is theoretically not needed but is only there for numerical stability (avoid div by 0) - due to the\n",
    "        # possibility of the computer rounding a very small number all the way to 0.\n",
    "        attentions_per_edge = exp_scores_per_edge / (neigborhood_aware_denominator + 1e-16)\n",
    "\n",
    "        # shape = (E, NH) -> (E, NH, 1) so that we can do element-wise multiplication with projected node features\n",
    "        return attentions_per_edge.unsqueeze(-1)\n",
    "\n",
    "    def sum_edge_scores_neighborhood_aware(self, exp_scores_per_edge, trg_index, num_of_nodes):\n",
    "        # The shape must be the same as in exp_scores_per_edge (required by scatter_add_) i.e. from E -> (E, NH)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(trg_index, exp_scores_per_edge)\n",
    "\n",
    "        # shape = (N, NH), where N is the number of nodes and NH the number of attention heads\n",
    "        size = list(exp_scores_per_edge.shape)  # convert to list otherwise assignment is not possible\n",
    "        size[self.nodes_dim] = num_of_nodes\n",
    "        neighborhood_sums = torch.zeros(size, dtype=exp_scores_per_edge.dtype, device=exp_scores_per_edge.device)\n",
    "\n",
    "        # position i will contain a sum of exp scores of all the nodes that point to the node i (as dictated by the\n",
    "        # target index)\n",
    "        neighborhood_sums.scatter_add_(self.nodes_dim, trg_index_broadcasted, exp_scores_per_edge)\n",
    "\n",
    "        # Expand again so that we can use it as a softmax denominator. e.g. node i's sum will be copied to\n",
    "        # all the locations where the source nodes pointed to i (as dictated by the target index)\n",
    "        # shape = (N, NH) -> (E, NH)\n",
    "        return neighborhood_sums.index_select(self.nodes_dim, trg_index)\n",
    "\n",
    "    def aggregate_neighbors(self, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes):\n",
    "        size = list(nodes_features_proj_lifted_weighted.shape)  # convert to list otherwise assignment is not possible\n",
    "        size[self.nodes_dim] = num_of_nodes  # shape = (N, NH, FOUT)\n",
    "        out_nodes_features = torch.zeros(size, dtype=in_nodes_features.dtype, device=in_nodes_features.device)\n",
    "\n",
    "        # shape = (E) -> (E, NH, FOUT)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(edge_index[self.trg_nodes_dim], nodes_features_proj_lifted_weighted)\n",
    "        # aggregation step - we accumulate projected, weighted node features for all the attention heads\n",
    "        # shape = (E, NH, FOUT) -> (N, NH, FOUT)\n",
    "        out_nodes_features.scatter_add_(self.nodes_dim, trg_index_broadcasted, nodes_features_proj_lifted_weighted)\n",
    "\n",
    "        return out_nodes_features\n",
    "\n",
    "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
    "        \"\"\"\n",
    "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
    "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
    "\n",
    "        \"\"\"\n",
    "        src_nodes_index = edge_index[self.src_nodes_dim]\n",
    "        trg_nodes_index = edge_index[self.trg_nodes_dim]\n",
    "\n",
    "        # Using index_select is faster than \"normal\" indexing (scores_source[src_nodes_index]) in PyTorch!\n",
    "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
    "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
    "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
    "\n",
    "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
    "\n",
    "    def explicit_broadcast(self, this, other):\n",
    "        # Append singleton dimensions until this.dim() == other.dim()\n",
    "        for _ in range(this.dim(), other.dim()):\n",
    "            this = this.unsqueeze(-1)\n",
    "\n",
    "        # Explicitly expand so that shapes are the same\n",
    "        return this.expand_as(other)\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\"\n",
    "        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n",
    "            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n",
    "\n",
    "        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n",
    "        Feel free to experiment - there may be better initializations depending on your problem.\n",
    "\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "    def skip_concat_bias(self, attention_coefficients, in_nodes_features, out_nodes_features):\n",
    "        if self.log_attention_weights:  # potentially log for later visualization in playground.py\n",
    "            self.attention_weights = attention_coefficients\n",
    "\n",
    "        if self.add_skip_connection:  # add skip or residual connection\n",
    "            if out_nodes_features.shape[-1] == in_nodes_features.shape[-1]:  # if FIN == FOUT\n",
    "                # unsqueeze does this: (N, FIN) -> (N, 1, FIN), out features are (N, NH, FOUT) so 1 gets broadcast to NH\n",
    "                # thus we're basically copying input vectors NH times and adding to processed vectors\n",
    "                out_nodes_features += in_nodes_features.unsqueeze(1)\n",
    "            else:\n",
    "                # FIN != FOUT so we need to project input feature vectors into dimension that can be added to output\n",
    "                # feature vectors. skip_proj adds lots of additional capacity which may cause overfitting.\n",
    "                out_nodes_features += self.skip_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        if self.concat:\n",
    "            # shape = (N, NH, FOUT) -> (N, NH*FOUT)\n",
    "            out_nodes_features = out_nodes_features.view(-1, self.num_of_heads * self.num_out_features)\n",
    "        else:\n",
    "            # shape = (N, NH, FOUT) -> (N, FOUT)\n",
    "            out_nodes_features = out_nodes_features.mean(dim=self.head_dim)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out_nodes_features += self.bias\n",
    "\n",
    "        return out_nodes_features if self.activation is None else self.activation(out_nodes_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-cemetery",
   "metadata": {},
   "source": [
    "The main idea that leads to huge savings is that we calculate the scores only for the nodes that will actually be used and not for every imaginable combination (that would be valid only in a fully-connected graph).\n",
    "\n",
    "Once we compute the `\"left\"` scores and the `\"right\"` scores, we \"lift\" them up using the edge index. That way\n",
    "if the edge `1->2` is not present in the graph we won't have those score pairs in our data structure.\n",
    "\n",
    "After adding lifted \"left\" and \"right\" (or maybe a better naming would be source and target) scores we do smart `neighborhood-aware softmax` - so that the semantics of GAT is respected. After doing the `scatter add` (which you should take your time to understand and go through the docs) we can combine the projected feature vectors, and voil√†, we got ourselves a fully-blown GAT layer.\n",
    "\n",
    "---\n",
    "\n",
    "Take your time and **be patient**! Especially if you're new to GNNs. \n",
    "\n",
    "I didn't learn all of this in 1 day, it takes time for the knowledge to sink in. You'll get there as well! ‚ù§Ô∏è (if you're not already there üòú)\n",
    "\n",
    "Having said that, we've got the level 3 unlocked (model training üí™). üòç\n",
    "\n",
    "We have the data üìú ready, we have the GAT model ü¶Ñ ready, let's start training this beast! üí™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-involvement",
   "metadata": {},
   "source": [
    "# Part 3: Training GAT üí™ (Multi-label classification on PPI!)\n",
    "\n",
    "Phew, well the hardest part is behind us. Let's know create a simple training loop where the goal is to learn to assign multiple labels to PPI nodes.\n",
    "\n",
    "But first let's define some relevant constants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dense-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# 3 different model training/eval phases used in train.py\n",
    "class LoopPhase(enum.Enum):\n",
    "    TRAIN = 0,\n",
    "    VAL = 1,\n",
    "    TEST = 2\n",
    "\n",
    "    \n",
    "writer = SummaryWriter()  # (tensorboard) writer will output to ./runs/ directory by default\n",
    "\n",
    "\n",
    "# Global vars used for early stopping. After some number of epochs (as defined by the patience_period var) without any\n",
    "# improvement on the validation dataset (measured via micro-F1 metric), we'll break out from the training loop.\n",
    "BEST_VAL_MICRO_F1 = 0\n",
    "BEST_VAL_LOSS = 0\n",
    "PATIENCE_CNT = 0\n",
    "\n",
    "BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries')\n",
    "CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints')\n",
    "\n",
    "# Make sure these exist as the rest of the code assumes it\n",
    "os.makedirs(BINARIES_PATH, exist_ok=True)\n",
    "os.makedirs(CHECKPOINTS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-ebony",
   "metadata": {},
   "source": [
    "Also, let's define a couple of functions that will be useful while training the model.\n",
    "\n",
    "The training state contains a lot of useful `metadata` which we can later use. You can imagine that saving the test accuracy of your model is important, especially when you're training your models on a cloud - it makes the organization so much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cardiac-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import re  # regex\n",
    "\n",
    "\n",
    "def get_training_state(training_config, model):\n",
    "    training_state = {\n",
    "        \"commit_hash\": git.Repo(search_parent_directories=True).head.object.hexsha,\n",
    "\n",
    "        # Training details\n",
    "        \"dataset_name\": training_config['dataset_name'],\n",
    "        \"num_of_epochs\": training_config['num_of_epochs'],\n",
    "        \"test_perf\": training_config['test_perf'],\n",
    "\n",
    "        # Model structure\n",
    "        \"num_of_layers\": training_config['num_of_layers'],\n",
    "        \"num_heads_per_layer\": training_config['num_heads_per_layer'],\n",
    "        \"num_features_per_layer\": training_config['num_features_per_layer'],\n",
    "        \"add_skip_connection\": training_config['add_skip_connection'],\n",
    "        \"bias\": training_config['bias'],\n",
    "        \"dropout\": training_config['dropout'],\n",
    "\n",
    "        # Model state\n",
    "        \"state_dict\": model.state_dict()\n",
    "    }\n",
    "\n",
    "    return training_state\n",
    "\n",
    "\n",
    "def print_model_metadata(training_state):\n",
    "    header = f'\\n{\"*\"*5} Model training metadata: {\"*\"*5}'\n",
    "    print(header)\n",
    "\n",
    "    for key, value in training_state.items():\n",
    "        if key != 'state_dict':  # don't print state_dict it's a bunch of numbers...\n",
    "            print(f'{key}: {value}')\n",
    "    print(f'{\"*\" * len(header)}\\n')\n",
    "    \n",
    "\n",
    "# This one makes sure we don't overwrite the valuable model binaries (feel free to ignore - not crucial to GAT method)\n",
    "def get_available_binary_name(dataset_name='unknown'):\n",
    "    prefix = f'gat_{dataset_name}'\n",
    "\n",
    "    def valid_binary_name(binary_name):\n",
    "        # First time you see raw f-string? Don't worry the only trick is to double the brackets.\n",
    "        pattern = re.compile(rf'{prefix}_[0-9]{{6}}\\.pth')\n",
    "        return re.fullmatch(pattern, binary_name) is not None\n",
    "\n",
    "    # Just list the existing binaries so that we don't overwrite them but write to a new one\n",
    "    valid_binary_names = list(filter(valid_binary_name, os.listdir(BINARIES_PATH)))\n",
    "    if len(valid_binary_names) > 0:\n",
    "        last_binary_name = sorted(valid_binary_names)[-1]\n",
    "        new_suffix = int(last_binary_name.split('.')[0][-6:]) + 1  # increment by 1\n",
    "        return f'{prefix}_{str(new_suffix).zfill(6)}.pth'\n",
    "    else:\n",
    "        return f'{prefix}_000000.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-brother",
   "metadata": {},
   "source": [
    "Nice, now `argparse` is just a nice way to **organize** your program settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "def get_training_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Training related\n",
    "    parser.add_argument(\"--num_of_epochs\", type=int, help=\"number of training epochs\", default=60)\n",
    "    parser.add_argument(\"--patience_period\", type=int, help=\"number of epochs with no improvement on val before terminating\", default=100)\n",
    "    parser.add_argument(\"--lr\", type=float, help=\"model learning rate\", default=5e-3)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, help=\"L2 regularization on model weights\", default=0)\n",
    "    parser.add_argument(\"--should_test\", type=bool, help='should test the model on the test dataset?', default=True)\n",
    "    parser.add_argument(\"--force_cpu\", type=bool, help='use CPU if your GPU is too small', default=False)\n",
    "\n",
    "    # Dataset related (note: we need the dataset name for metadata and related stuff, and not for picking the dataset)\n",
    "    parser.add_argument(\"--dataset_name\", choices=[el.name for el in DatasetType], help='dataset to use for training', default=DatasetType.PPI.name)\n",
    "    parser.add_argument(\"--batch_size\", type=int, help='number of graphs in a batch', default=2)\n",
    "    parser.add_argument(\"--should_visualize\", type=bool, help='should visualize the dataset?', default=False)\n",
    "\n",
    "    # Logging/debugging/checkpoint related (helps a lot with experimentation)\n",
    "    parser.add_argument(\"--enable_tensorboard\", type=bool, help=\"enable tensorboard logging\", default=False)\n",
    "    parser.add_argument(\"--console_log_freq\", type=int, help=\"log to output console (epoch) freq (None for no logging)\", default=10)\n",
    "    parser.add_argument(\"--checkpoint_freq\", type=int, help=\"checkpoint model saving (epoch) freq (None for no logging)\", default=5)\n",
    "    args = parser.parse_args('')\n",
    "\n",
    "    # I'm leaving the hyperparam values as reported in the paper, but I experimented a bit and the comments suggest\n",
    "    # how you can make GAT achieve an even higher micro-F1 or make it smaller\n",
    "    gat_config = {\n",
    "        # GNNs, contrary to CNNs, are often shallow (it ultimately depends on the graph properties)\n",
    "        #\"num_of_layers\": 3,  # PPI has got 42% of nodes with all 0 features - that's why 3 layers are useful\n",
    "        \"num_of_layers\": 1,\n",
    "        #\"num_heads_per_layer\": [4, 4, 6],  # other values may give even better results from the reported ones\n",
    "        \"num_heads_per_layer\": [8],\n",
    "        #\"num_features_per_layer\": [PPI_NUM_INPUT_FEATURES, 64, 64, PPI_NUM_CLASSES],  # 64 would also give ~0.975 uF1!\n",
    "        \"num_features_per_layer\": [PPI_NUM_INPUT_FEATURES, PPI_NUM_CLASSES],\n",
    "        #\"add_skip_connection\": True,  # skip connection is very important! (keep it otherwise micro-F1 is almost 0)\n",
    "        \"add_skip_connection\": False,\n",
    "        \"bias\": False,  # bias doesn't matter that much\n",
    "        \"dropout\": 0.0,  # dropout hurts the performance (best to keep it at 0)\n",
    "    }\n",
    "\n",
    "    # Wrapping training configuration into a dictionary\n",
    "    training_config = dict()\n",
    "    for arg in vars(args):\n",
    "        training_config[arg] = getattr(args, arg)\n",
    "    training_config['ppi_load_test_only'] = False  # load both train/val/test data loaders (don't change it)\n",
    "\n",
    "    # Add additional config information\n",
    "    training_config.update(gat_config)\n",
    "\n",
    "    return training_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-stanford",
   "metadata": {},
   "source": [
    "Now for the juicy part. üç™üéÖ\n",
    "\n",
    "Here, we organize, high-level, everything we need for training GAT. Just combining the components we already learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train_gat_ppi(config):\n",
    "    \"\"\"\n",
    "    Very similar to Cora's training script. The main differences are:\n",
    "    1. Using dataloaders since we're dealing with an inductive setting - multiple graphs per batch\n",
    "    2. Doing multi-class classification (BCEWithLogitsLoss) and reporting micro-F1 instead of accuracy\n",
    "    3. Model architecture and hyperparams are a bit different (as reported in the GAT paper)\n",
    "\n",
    "    \"\"\"\n",
    "    global BEST_VAL_MICRO_F1, BEST_VAL_LOSS\n",
    "\n",
    "    # Checking whether you have a strong GPU. Since PPI training requires almost 8 GBs of VRAM\n",
    "    # I've added the option to force the use of CPU even though you have a GPU on your system (but it's too weak).\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not config['force_cpu'] else \"cpu\")\n",
    "\n",
    "    # Step 1: prepare the data loaders\n",
    "    data_loader_train, data_loader_val, data_loader_test = load_graph_data(config, device)\n",
    "\n",
    "    # Step 2: prepare the model\n",
    "    gat = GAT(\n",
    "        num_of_layers=config['num_of_layers'],\n",
    "        num_heads_per_layer=config['num_heads_per_layer'],\n",
    "        num_features_per_layer=config['num_features_per_layer'],\n",
    "        add_skip_connection=config['add_skip_connection'],\n",
    "        bias=config['bias'],\n",
    "        dropout=config['dropout'],\n",
    "        log_attention_weights=False  # no need to store attentions, used only in playground.py for visualizations\n",
    "    ).to(device)\n",
    "\n",
    "    # Step 3: Prepare other training related utilities (loss & optimizer and decorator function)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    optimizer = Adam(gat.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "    # The decorator function makes things cleaner since there is a lot of redundancy between the train and val loops\n",
    "    main_loop = get_main_loop(\n",
    "        config,\n",
    "        gat,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        config['patience_period'],\n",
    "        time.time())\n",
    "\n",
    "    BEST_VAL_MICRO_F1, BEST_VAL_LOSS, PATIENCE_CNT = [0, 0, 0]  # reset vars used for early stopping\n",
    "\n",
    "    # Step 4: Start the training procedure\n",
    "    for epoch in range(config['num_of_epochs']):\n",
    "        # Training loop\n",
    "        main_loop(phase=LoopPhase.TRAIN, data_loader=data_loader_train, epoch=epoch)\n",
    "\n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                main_loop(phase=LoopPhase.VAL, data_loader=data_loader_val, epoch=epoch)\n",
    "            except Exception as e:  # \"patience has run out\" exception :O\n",
    "                print(str(e))\n",
    "                break  # break out from the training loop\n",
    "\n",
    "    # Step 5: Potentially test your model\n",
    "    # Don't overfit to the test dataset - only when you've fine-tuned your model on the validation dataset should you\n",
    "    # report your final loss and micro-F1 on the test dataset. Friends don't let friends overfit to the test data. <3\n",
    "    if config['should_test']:\n",
    "        micro_f1 = main_loop(phase=LoopPhase.TEST, data_loader=data_loader_test)\n",
    "        config['test_perf'] = micro_f1\n",
    "\n",
    "        print('*' * 50)\n",
    "        print(f'Test micro-F1 = {micro_f1}')\n",
    "    else:\n",
    "        config['test_perf'] = -1\n",
    "\n",
    "    # Save the latest GAT in the binaries directory\n",
    "    torch.save(\n",
    "        get_training_state(config, gat),\n",
    "        os.path.join(BINARIES_PATH, get_available_binary_name(config['dataset_name']))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-grenada",
   "metadata": {},
   "source": [
    "üéâüéâüéâ \n",
    "\n",
    "Now for the core part of the training - the main loop, as I've dubbed it. \n",
    "\n",
    "I've organized it like this so that I don't have to copy/paste bunch of the same code for train/val/test loops.\n",
    "\n",
    "**Friends don't let friends copy/paste (unless it's from the Stack Overflow)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Simple decorator function so that I don't have to pass arguments that don't change from epoch to epoch\n",
    "def get_main_loop(config, gat, sigmoid_cross_entropy_loss, optimizer, patience_period, time_start):\n",
    "\n",
    "    device = next(gat.parameters()).device  # fetch the device info from the model instead of passing it as a param\n",
    "\n",
    "    def main_loop(phase, data_loader, epoch=0):\n",
    "        global BEST_VAL_MICRO_F1, BEST_VAL_LOSS, PATIENCE_CNT, writer\n",
    "\n",
    "        # Certain modules behave differently depending on whether we're training the model or not.\n",
    "        # e.g. nn.Dropout - we only want to drop model weights during the training.\n",
    "        if phase == LoopPhase.TRAIN:\n",
    "            gat.train()\n",
    "        else:\n",
    "            gat.eval()\n",
    "\n",
    "        # Iterate over batches of graph data (2 graphs per batch was used in the original paper for the PPI dataset)\n",
    "        # We merge them into a single graph with 2 connected components, that's the main idea. After that\n",
    "        # the implementation #3 is agnostic to the fact that those are multiple and not a single graph!\n",
    "        for batch_idx, (node_features, gt_node_labels, edge_index) in enumerate(data_loader):\n",
    "            # Push the batch onto GPU - note PPI is to big to load the whole dataset into a normal GPU\n",
    "            # it takes almost 8 GBs of VRAM to train it on a GPU\n",
    "            edge_index = edge_index.to(device)\n",
    "            node_features = node_features.to(device)\n",
    "            gt_node_labels = gt_node_labels.to(device)\n",
    "\n",
    "            # I pack data into tuples because GAT uses nn.Sequential which expects this format\n",
    "            graph_data = (node_features, edge_index)\n",
    "\n",
    "            # Note: [0] just extracts the node_features part of the data (index 1 contains the edge_index)\n",
    "            # shape = (N, C) where N is the number of nodes in the batch and C is the number of classes (121 for PPI)\n",
    "            # GAT imp #3 is agnostic to the fact that we actually have multiple graphs\n",
    "            # (it sees a single graph with multiple connected components)\n",
    "            nodes_unnormalized_scores = gat(graph_data)[0]\n",
    "\n",
    "            # Example: because PPI has 121 labels let's make a simple toy example that will show how the loss works.\n",
    "            # Let's say we have 3 labels instead and a single node's unnormalized (raw GAT output) scores are [-3, 0, 3]\n",
    "            # What this loss will do is first it will apply a sigmoid and so we'll end up with: [0.048, 0.5, 0.95]\n",
    "            # next it will apply a binary cross entropy across all of these and find the average, and that's it!\n",
    "            # So if the true classes were [0, 0, 1] the loss would be (-log(1-0.048) + -log(1-0.5) + -log(0.95))/3.\n",
    "            # You can see that the logarithm takes 2 forms depending on whether the true label is 0 or 1,\n",
    "            # either -log(1-x) or -log(x) respectively. Easy-peasy. <3\n",
    "            loss = sigmoid_cross_entropy_loss(nodes_unnormalized_scores, gt_node_labels)\n",
    "\n",
    "            if phase == LoopPhase.TRAIN:\n",
    "                optimizer.zero_grad()  # clean the trainable weights gradients in the computational graph (.grad fields)\n",
    "                loss.backward()  # compute the gradients for every trainable weight in the computational graph\n",
    "                optimizer.step()  # apply the gradients to weights\n",
    "\n",
    "            # Calculate the main metric - micro F1, check out this link for what micro-F1 exactly is:\n",
    "            # https://www.kaggle.com/enforcer007/what-is-micro-averaged-f1-score\n",
    "\n",
    "            # Convert unnormalized scores into predictions. Explanation:\n",
    "            # If the unnormalized score is bigger than 0 that means that sigmoid would have a value higher than 0.5\n",
    "            # (by sigmoid's definition) and thus we have predicted 1 for that label otherwise we have predicted 0.\n",
    "            pred = (nodes_unnormalized_scores > 0).float().cpu().numpy()\n",
    "            gt = gt_node_labels.cpu().numpy()\n",
    "            micro_f1 = f1_score(gt, pred, average='micro')\n",
    "\n",
    "            #\n",
    "            # Logging\n",
    "            #\n",
    "\n",
    "            global_step = len(data_loader) * epoch + batch_idx\n",
    "            if phase == LoopPhase.TRAIN:\n",
    "                # Log metrics\n",
    "                if config['enable_tensorboard']:\n",
    "                    writer.add_scalar('training_loss', loss.item(), global_step)\n",
    "                    writer.add_scalar('training_micro_f1', micro_f1, global_step)\n",
    "\n",
    "                # Log to console\n",
    "                if config['console_log_freq'] is not None and epoch % config['console_log_freq'] == 0 and batch_idx == 0:\n",
    "                    print(f'GAT training: time elapsed= {(time.time() - time_start):.2f} [s] |'\n",
    "                          f' epoch={epoch + 1} | batch={batch_idx + 1} | train micro-F1={micro_f1}.')\n",
    "\n",
    "                # Save model checkpoint\n",
    "                if config['checkpoint_freq'] is not None and (epoch + 1) % config['checkpoint_freq'] == 0 and batch_idx == 0:\n",
    "                    ckpt_model_name = f'gat_{config[\"dataset_name\"]}_ckpt_epoch_{epoch + 1}.pth'\n",
    "                    config['test_perf'] = -1  # test perf not calculated yet, note: perf means main metric micro-F1 here\n",
    "                    torch.save(get_training_state(config, gat), os.path.join(CHECKPOINTS_PATH, ckpt_model_name))\n",
    "\n",
    "            elif phase == LoopPhase.VAL:\n",
    "                # Log metrics\n",
    "                if config['enable_tensorboard']:\n",
    "                    writer.add_scalar('val_loss', loss.item(), global_step)\n",
    "                    writer.add_scalar('val_micro_f1', micro_f1, global_step)\n",
    "\n",
    "                # Log to console\n",
    "                if config['console_log_freq'] is not None and epoch % config['console_log_freq'] == 0 and batch_idx == 0:\n",
    "                    print(f'GAT validation: time elapsed= {(time.time() - time_start):.2f} [s] |'\n",
    "                          f' epoch={epoch + 1} | batch={batch_idx + 1} | val micro-F1={micro_f1}')\n",
    "\n",
    "                # The \"patience\" logic - should we break out from the training loop? If either validation micro-F1\n",
    "                # keeps going up or the val loss keeps going down we won't stop\n",
    "                if micro_f1 > BEST_VAL_MICRO_F1 or loss.item() < BEST_VAL_LOSS:\n",
    "                    BEST_VAL_MICRO_F1 = max(micro_f1, BEST_VAL_MICRO_F1)  # keep track of the best validation micro_f1 so far\n",
    "                    BEST_VAL_LOSS = min(loss.item(), BEST_VAL_LOSS)  # and the minimal loss\n",
    "                    PATIENCE_CNT = 0  # reset the counter every time we encounter new best micro_f1\n",
    "                else:\n",
    "                    PATIENCE_CNT += 1  # otherwise keep counting\n",
    "\n",
    "                if PATIENCE_CNT >= patience_period:\n",
    "                    raise Exception('Stopping the training, the universe has no more patience for this training.')\n",
    "\n",
    "            else:\n",
    "                return micro_f1  # in the case of test phase we just report back the test micro_f1\n",
    "\n",
    "    return main_loop  # return the decorated function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-stupid",
   "metadata": {},
   "source": [
    "That was all we needed! Let's train it! üí™üí™üí™\n",
    "\n",
    "Keep in mind that PPI training takes much more time than training on Cora.\n",
    "\n",
    "Additionally you'll need 8+ GBs GPU if you want to train it on your GPU. Alternatively you can set the `--force_cpu` flag in the `get_training_args` function to train it on your CPU or simply use the pre-checked-in model I provided. The following section, part 4, doesn't depend on this so you can skip it if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the graph attention network (GAT)\n",
    "train_gat_ppi(get_training_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-weather",
   "metadata": {},
   "source": [
    "Nice!!! üéâüéâüéâ Level 4 unlocked (GAT visualizations üîÆ). üòç\n",
    "\n",
    "We just achieved `0.978` micro-F1 on PPI's test graphs! The same numbers as reported in the original GAT paper!\n",
    "\n",
    "So we now have everything in place:\n",
    "1. PPI data loading and visualizations üìú -> checked\n",
    "2. GAT model defined ü¶Ñ -> checked\n",
    "3. Training loop setup and the trained model binaries üí™ -> checked\n",
    "\n",
    "Now let's play the GAT model under a microscope üî¨üî¨üî¨ and understand the weights we got - we can do that in many ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-reach",
   "metadata": {},
   "source": [
    "# Part 4: Visualizing GAT on PPI üîÆ\n",
    "\n",
    "I tried visualizing PPI's 2D embeddings using t-SNE without any label/color information but it's not that informative, so we'll only do **attention** and **entropy visualizations** in this notebook.\n",
    "\n",
    "Let's start by defining some functions we'll need. \n",
    "\n",
    "The following cell's code snippet will get called multiple times so let's just extract it inside a function - a nice modular design.\n",
    "\n",
    "*Note: the main reason is actually that igraph is having problems with Jupyter so I'm working around it, check out the [original code](https://github.com/gordicaleksa/pytorch-GAT/blob/main/playground.py#L147) if you're curious* üòÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "satisfactory-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gat_forward_pass(model_name, dataset_name):\n",
    "    device = torch.device(\"cpu\")  # checking whether you have a GPU, I hope so!\n",
    "\n",
    "    config = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'should_visualize': False,  # don't visualize the dataset\n",
    "        'batch_size': 2,  # we're using 2 graphs per batch as reported in the paper\n",
    "        'ppi_load_test_only': True  # optimization, we're loading only test graphs\n",
    "    }\n",
    "\n",
    "    # Step 1: Prepare the data\n",
    "    data_loader_test = load_graph_data(config, device)\n",
    "    node_features, node_labels, topology = next(iter(data_loader_test))\n",
    "    node_features = node_features.to(device)  # need to explicitly push them to GPU since PPI eats up a lot of VRAM\n",
    "    node_labels = node_labels.to(device)\n",
    "    topology = topology.to(device)\n",
    "\n",
    "    # Step 2: Prepare the model\n",
    "    model_path = os.path.join(BINARIES_PATH, model_name)\n",
    "    model_state = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "    gat = GAT(\n",
    "        num_of_layers=model_state['num_of_layers'],\n",
    "        num_heads_per_layer=model_state['num_heads_per_layer'],\n",
    "        num_features_per_layer=model_state['num_features_per_layer'],\n",
    "        add_skip_connection=model_state['add_skip_connection'],\n",
    "        bias=model_state['bias'],\n",
    "        dropout=model_state['dropout'],\n",
    "        log_attention_weights=True\n",
    "    ).to(device)\n",
    "\n",
    "    print_model_metadata(model_state)\n",
    "    assert model_state['dataset_name'].lower() == dataset_name.lower(), \\\n",
    "        f\"The model was trained on {model_state['dataset_name']} but you're calling it on {dataset_name}.\"\n",
    "    gat.load_state_dict(model_state[\"state_dict\"], strict=True)\n",
    "    gat.eval()  # some layers like nn.Dropout behave differently in train vs eval mode so this part is important\n",
    "\n",
    "    # Step 3: Calculate the things we'll need for different visualization types (attention, scores, edge_index)\n",
    "\n",
    "    # This context manager is important (and you'll often see it), otherwise PyTorch will eat much more memory.\n",
    "    # It would be saving activations for backprop but we are not going to do any model training just the prediction.\n",
    "    with torch.no_grad():\n",
    "        # Step 3: Run predictions and collect the high dimensional data\n",
    "        all_nodes_unnormalized_scores, _ = gat((node_features, topology))  # shape = (N, num of classes)\n",
    "        all_nodes_unnormalized_scores = all_nodes_unnormalized_scores.cpu().numpy()\n",
    "\n",
    "    return all_nodes_unnormalized_scores, topology, node_labels, gat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-drain",
   "metadata": {},
   "source": [
    "Nice that one just produces the data that'll get consumed in the downstream visualizations that you'll see defined in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-tamil",
   "metadata": {},
   "source": [
    "# Visualizing neighborhood attention üì£\n",
    "\n",
    "So, you now hopefully understand how GAT roughly works, and so you know that during the aggregation stage every single node assigns an **attention coefficient** to every single one of its neighbors (including itself since we added self edges).\n",
    "\n",
    "Any ideas on what we could visualize? Well let's pick some nodes and see which attention patterns they've learned!\n",
    "\n",
    "The first idea that may pop to your mind is to draw edges **thicker** if the **attention is larger** and vice versa (*well that's also the last idea that pops to my mind*).\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "combined-ceiling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test graph 23 to CPU. It has 3224 nodes and 103872 edges.\n",
      "Loading test graph 24 to CPU. It has 2300 nodes and 63628 edges.\n",
      "\n",
      "***** Model training metadata: *****\n",
      "commit_hash: fc0d286f4566a311134869470d0ee9c88b7f1bce\n",
      "dataset_name: PPI\n",
      "num_of_epochs: 200\n",
      "test_perf: 0.6010310668414757\n",
      "num_of_layers: 1\n",
      "num_heads_per_layer: [8]\n",
      "num_features_per_layer: [50, 121]\n",
      "add_skip_connection: False\n",
      "bias: False\n",
      "dropout: 0.0\n",
      "*************************************\n",
      "\n",
      "features_proj (node 0, head 1) =  tensor([-1.0884,  0.1125, -4.2577,  0.6560, -4.1832, -1.0960,  0.1966, -0.8447,\n",
      "        -6.6560, -2.8093, -5.4067,  2.9334,  2.3171, -1.2962, -3.7417, -2.5590,\n",
      "        -6.1468, -3.4637, -2.8742, -3.1723, -0.2643, -0.4037, -3.0179, -7.3067,\n",
      "        -2.5935, -2.7346, -4.5094, -0.4673, -5.5418, -2.6939, -0.9223,  0.0310,\n",
      "        -0.7240, -1.7213, -2.9464, -2.8111,  1.8250, -7.3265, -4.3719, -5.0126,\n",
      "        -1.5536,  1.6187, -2.4586, -1.6744,  4.3152, -2.6729, -2.2695, -3.0516,\n",
      "        -1.8156, -2.2391, -3.9844,  1.6768,  1.4001, -3.4711,  2.4053, -6.2201,\n",
      "        -1.5362, -2.5004, -7.9084, -5.2763, -0.1621, -2.8752, -2.8582, -4.9710,\n",
      "        -1.6646, -1.3097, -1.1551,  1.8518, -0.1934, -1.8730, -1.1561, -1.8321,\n",
      "        -6.3864, -4.3680, -2.5810, -0.8340, -7.5175, -2.9907, -2.9385, -3.7279,\n",
      "        -2.6305, -1.7574, -5.3045,  0.1573,  0.6269, -1.9216, -6.3444, -4.6213,\n",
      "        -1.3846, -6.5291,  1.5358, -5.2737, -3.1468, -5.1765,  1.3467, -1.8336,\n",
      "        -3.0847, -2.6019, -4.8709, -1.0893, -1.1252,  1.6989,  0.8452, -6.2998,\n",
      "        -2.5338, -2.9872, -0.9663, -7.3541, -2.5092, -0.2966, -1.6063, -3.3427,\n",
      "        -3.1811, -4.3503, -7.7468, -3.8393, -6.4464, -5.3570, -5.4650, -3.8155,\n",
      "        -2.4132])\n",
      "source node ids = [   0  163  165  569  633  634  685  702  793  796 1230 1804 1864 2350\n",
      " 2593 2746 2771 3000 3108 3149]\n",
      "attention weights = [4.2532803e-03 4.2049745e-05 1.0969959e-05 2.1232719e-02 5.6562810e-03\n",
      " 4.4813366e-03 8.8380910e-03 4.4813366e-03 8.1054326e-03 1.0287648e-05\n",
      " 3.2607049e-03 3.2607049e-03 1.4690934e-02 1.2125604e-05 3.2607049e-03\n",
      " 7.1131180e-06 3.2607049e-03 4.4813366e-03 3.2607049e-03 7.2690891e-06]\n",
      "Max attention weight = 0.021232718601822853 and min = 7.113118044799194e-06\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600pt\" height=\"600pt\" viewBox=\"0 0 600 600\" version=\"1.1\">\n",
       "<g id=\"surface56\">\n",
       "<rect x=\"0\" y=\"0\" width=\"600\" height=\"600\" style=\"fill:rgb(100%,100%,100%);fill-opacity:1;stroke:none;\"/>\n",
       "<path style=\"fill:none;stroke-width:0.200317;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 323.871094 289.585938 C 323.871094 297.871094 317.15625 304.585938 308.871094 304.585938 C 300.589844 304.585938 293.871094 297.871094 293.871094 289.585938 C 293.871094 281.300781 300.589844 274.585938 308.871094 274.585938 C 317.15625 274.585938 323.871094 281.300781 323.871094 289.585938 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.00198042;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 580 300.191406 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.000516654;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 564.652344 391.636719 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 520.273438 473.117188 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.266395;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 451.710938 535.753906 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.211058;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 366.421875 572.726562 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.416249;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 273.710938 580 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.211058;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 183.675781 556.785156 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.381743;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 106.125 505.613281 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.000484519;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 49.507812 432.054688 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.15357;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 20 344.132812 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.15357;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 20.8125 251.417969 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.691901;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 51.855469 164.019531 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.000571081;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 109.75 91.460938 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.15357;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 188.183594 41.644531 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.000335007;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 278.613281 20 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.15357;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 371.183594 28.886719 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.211058;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 455.808594 67.335938 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.15357;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 523.269531 131.15625 \"/>\n",
       "<path style=\"fill:none;stroke-width:0.000342353;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.265625 300.191406 L 566.210938 213.398438 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 308.265625 300.191406 C 308.265625 305.714844 303.789062 310.191406 298.265625 310.191406 C 292.742188 310.191406 288.265625 305.714844 288.265625 300.191406 C 288.265625 294.667969 292.742188 290.191406 298.265625 290.191406 C 303.789062 290.191406 308.265625 294.667969 308.265625 300.191406 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 590 300.191406 C 590 305.714844 585.523438 310.191406 580 310.191406 C 574.476562 310.191406 570 305.714844 570 300.191406 C 570 294.667969 574.476562 290.191406 580 290.191406 C 585.523438 290.191406 590 294.667969 590 300.191406 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 574.652344 391.636719 C 574.652344 397.160156 570.171875 401.636719 564.652344 401.636719 C 559.128906 401.636719 554.652344 397.160156 554.652344 391.636719 C 554.652344 386.113281 559.128906 381.636719 564.652344 381.636719 C 570.171875 381.636719 574.652344 386.113281 574.652344 391.636719 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 530.273438 473.117188 C 530.273438 478.640625 525.796875 483.117188 520.273438 483.117188 C 514.753906 483.117188 510.273438 478.640625 510.273438 473.117188 C 510.273438 467.59375 514.753906 463.117188 520.273438 463.117188 C 525.796875 463.117188 530.273438 467.59375 530.273438 473.117188 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 461.710938 535.753906 C 461.710938 541.277344 457.230469 545.753906 451.710938 545.753906 C 446.1875 545.753906 441.710938 541.277344 441.710938 535.753906 C 441.710938 530.230469 446.1875 525.753906 451.710938 525.753906 C 457.230469 525.753906 461.710938 530.230469 461.710938 535.753906 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 376.421875 572.726562 C 376.421875 578.25 371.945312 582.726562 366.421875 582.726562 C 360.902344 582.726562 356.421875 578.25 356.421875 572.726562 C 356.421875 567.203125 360.902344 562.726562 366.421875 562.726562 C 371.945312 562.726562 376.421875 567.203125 376.421875 572.726562 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 283.710938 580 C 283.710938 585.523438 279.234375 590 273.710938 590 C 268.1875 590 263.710938 585.523438 263.710938 580 C 263.710938 574.476562 268.1875 570 273.710938 570 C 279.234375 570 283.710938 574.476562 283.710938 580 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 193.675781 556.785156 C 193.675781 562.308594 189.195312 566.785156 183.675781 566.785156 C 178.152344 566.785156 173.675781 562.308594 173.675781 556.785156 C 173.675781 551.261719 178.152344 546.785156 183.675781 546.785156 C 189.195312 546.785156 193.675781 551.261719 193.675781 556.785156 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 116.125 505.613281 C 116.125 511.136719 111.644531 515.613281 106.125 515.613281 C 100.601562 515.613281 96.125 511.136719 96.125 505.613281 C 96.125 500.089844 100.601562 495.613281 106.125 495.613281 C 111.644531 495.613281 116.125 500.089844 116.125 505.613281 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 59.507812 432.054688 C 59.507812 437.578125 55.03125 442.054688 49.507812 442.054688 C 43.984375 442.054688 39.507812 437.578125 39.507812 432.054688 C 39.507812 426.53125 43.984375 422.054688 49.507812 422.054688 C 55.03125 422.054688 59.507812 426.53125 59.507812 432.054688 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 30 344.132812 C 30 349.652344 25.523438 354.132812 20 354.132812 C 14.476562 354.132812 10 349.652344 10 344.132812 C 10 338.609375 14.476562 334.132812 20 334.132812 C 25.523438 334.132812 30 338.609375 30 344.132812 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 30.8125 251.417969 C 30.8125 256.941406 26.335938 261.417969 20.8125 261.417969 C 15.289062 261.417969 10.8125 256.941406 10.8125 251.417969 C 10.8125 245.894531 15.289062 241.417969 20.8125 241.417969 C 26.335938 241.417969 30.8125 245.894531 30.8125 251.417969 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 61.855469 164.019531 C 61.855469 169.542969 57.378906 174.019531 51.855469 174.019531 C 46.332031 174.019531 41.855469 169.542969 41.855469 164.019531 C 41.855469 158.496094 46.332031 154.019531 51.855469 154.019531 C 57.378906 154.019531 61.855469 158.496094 61.855469 164.019531 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 119.75 91.460938 C 119.75 96.984375 115.273438 101.460938 109.75 101.460938 C 104.226562 101.460938 99.75 96.984375 99.75 91.460938 C 99.75 85.9375 104.226562 81.460938 109.75 81.460938 C 115.273438 81.460938 119.75 85.9375 119.75 91.460938 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 198.183594 41.644531 C 198.183594 47.167969 193.707031 51.644531 188.183594 51.644531 C 182.660156 51.644531 178.183594 47.167969 178.183594 41.644531 C 178.183594 36.121094 182.660156 31.644531 188.183594 31.644531 C 193.707031 31.644531 198.183594 36.121094 198.183594 41.644531 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 288.613281 20 C 288.613281 25.523438 284.136719 30 278.613281 30 C 273.089844 30 268.613281 25.523438 268.613281 20 C 268.613281 14.476562 273.089844 10 278.613281 10 C 284.136719 10 288.613281 14.476562 288.613281 20 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 381.183594 28.886719 C 381.183594 34.410156 376.707031 38.886719 371.183594 38.886719 C 365.660156 38.886719 361.183594 34.410156 361.183594 28.886719 C 361.183594 23.363281 365.660156 18.886719 371.183594 18.886719 C 376.707031 18.886719 381.183594 23.363281 381.183594 28.886719 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 465.808594 67.335938 C 465.808594 72.859375 461.332031 77.335938 455.808594 77.335938 C 450.285156 77.335938 445.808594 72.859375 445.808594 67.335938 C 445.808594 61.8125 450.285156 57.335938 455.808594 57.335938 C 461.332031 57.335938 465.808594 61.8125 465.808594 67.335938 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 533.269531 131.15625 C 533.269531 136.679688 528.792969 141.15625 523.269531 141.15625 C 517.746094 141.15625 513.269531 136.679688 513.269531 131.15625 C 513.269531 125.632812 517.746094 121.15625 523.269531 121.15625 C 528.792969 121.15625 533.269531 125.632812 533.269531 131.15625 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 576.210938 213.398438 C 576.210938 218.917969 571.734375 223.398438 566.210938 223.398438 C 560.6875 223.398438 556.210938 218.917969 556.210938 213.398438 C 556.210938 207.875 560.6875 203.398438 566.210938 203.398438 C 571.734375 203.398438 576.210938 207.875 576.210938 213.398438 \"/>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<igraph.drawing.Plot at 0x7fb188addbe0>"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "image/svg+xml": {
       "isolated": true
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again, unfortunately, igraph is having some problems running in Jupyter so I have to flatten out the content here\n",
    "# including the for loops - no for loops with igraph in Jupyter folks.\n",
    "\n",
    "model_name=r'gat_PPI_000001.pth'  # This model is checked-in, feel free to use the one you trained\n",
    "dataset_name=DatasetType.PPI.name\n",
    "\n",
    "# Fetch the data we'll need to create visualizations\n",
    "all_nodes_unnormalized_scores, edge_index, node_labels, gat = gat_forward_pass(model_name, dataset_name)\n",
    "\n",
    "head_to_visualize = 0  # plot attention from this multi-head attention's head\n",
    "gat_layer_id = 0  # plot attention from this GAT layer\n",
    "\n",
    "assert gat_layer_id == 0, f'Attention visualization for {dataset_name} is only available for the first layer.'\n",
    "\n",
    "# Build up the complete graph\n",
    "# node_features shape = (N, FIN), where N is the number of nodes and FIN number of input features\n",
    "total_num_of_nodes = len(all_nodes_unnormalized_scores)\n",
    "complete_graph = ig.Graph()\n",
    "complete_graph.add_vertices(total_num_of_nodes)  # igraph creates nodes with ids [0, total_num_of_nodes - 1]\n",
    "edge_index_tuples = list(zip(edge_index[0, :], edge_index[1, :]))  # igraph requires this format\n",
    "complete_graph.add_edges(edge_index_tuples)\n",
    "\n",
    "target_node_ids = edge_index[1]\n",
    "source_nodes = edge_index[0]\n",
    "\n",
    "#\n",
    "# Pick the node id you want to visualize the attention for!\n",
    "#\n",
    "\n",
    "# since for loops won't work with igraph just set some number here\n",
    "target_node_id = 0  # random node\n",
    "\n",
    "# Step 1: Find the neighboring nodes to the target node\n",
    "# Note: self edges are included so the target node is it's own neighbor (Alexandro yo soy tu madre)\n",
    "src_nodes_indices = torch.eq(target_node_ids, target_node_id)\n",
    "source_node_ids = source_nodes[src_nodes_indices].cpu().numpy()\n",
    "print(\"source node ids =\", source_node_ids)\n",
    "size_of_neighborhood = len(source_node_ids)\n",
    "\n",
    "# Step 2: Fetch their labels\n",
    "labels = node_labels[source_node_ids].cpu().numpy()\n",
    "\n",
    "# Step 3: Fetch the attention weights for edges (attention is logged during GAT's forward pass above)\n",
    "# attention shape = (N, NH, 1) -> (N, NH) - we just squeeze the last dim it's superfluous\n",
    "all_attention_weights = gat.gat_net[gat_layer_id].attention_weights.squeeze(dim=-1)\n",
    "attention_weights = all_attention_weights[src_nodes_indices, head_to_visualize].cpu().numpy()\n",
    "print(\"attention weights =\", attention_weights)\n",
    "# PPI's attention pattern is much less uniform than Cora's\n",
    "print(f'Max attention weight = {np.max(attention_weights)} and min = {np.min(attention_weights)}')\n",
    "attention_weights /= np.max(attention_weights)  # rescale the biggest weight to 1 for nicer plotting\n",
    "\n",
    "# Build up the neighborhood graph whose attention we want to visualize\n",
    "# igraph constraint - it works with contiguous range of ids so we map e.g. node 497 to 0, 12 to 1, etc.\n",
    "id_to_igraph_id = dict(zip(source_node_ids, range(len(source_node_ids))))\n",
    "ig_graph = ig.Graph()\n",
    "ig_graph.add_vertices(size_of_neighborhood)\n",
    "ig_graph.add_edges([(id_to_igraph_id[neighbor], id_to_igraph_id[target_node_id]) for neighbor in source_node_ids])\n",
    "\n",
    "# Prepare the visualization settings dictionary and plot\n",
    "visual_style = {\n",
    "    \"edge_width\": attention_weights,  # make edges as thick as the corresponding attention weight\n",
    "    \"layout\": ig_graph.layout_reingold_tilford_circular()  # layout for tree-like graphs\n",
    "}\n",
    "\n",
    "ig.plot(ig_graph, **visual_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05f5471",
   "metadata": {},
   "source": [
    "# Post-processing for Parallel-GAT project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d032060",
   "metadata": {},
   "source": [
    "## Export Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e36162a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=r'models/binaries/gat_PPI_000001.pth'\n",
    "model_state = torch.load(model_name, map_location=torch.device(\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f4253d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'commit_hash': 'fc0d286f4566a311134869470d0ee9c88b7f1bce', 'dataset_name': 'PPI', 'num_of_epochs': 200, 'test_perf': 0.6010310668414757, 'num_of_layers': 1, 'num_heads_per_layer': [8], 'num_features_per_layer': [50, 121], 'add_skip_connection': False, 'bias': False, 'dropout': 0.0, 'state_dict': OrderedDict([('gat_net.0.scoring_fn_target', tensor([[[ 4.2526e-02, -2.2061e-02,  1.7813e-01,  9.5327e-02,  1.9419e-02,\n",
      "          -1.3862e-02,  9.5293e-02, -7.1660e-02,  9.5219e-02,  2.0901e-03,\n",
      "           5.1543e-02,  1.7303e-02,  2.4387e-01,  3.5561e-02, -3.2772e-02,\n",
      "           5.8154e-02, -7.1884e-02, -2.8155e-02,  2.3671e-02, -2.3235e-02,\n",
      "          -4.7205e-02,  1.7999e-02,  1.1128e-02,  6.4638e-02, -4.2352e-02,\n",
      "           1.5734e-01,  2.4488e-01,  5.5371e-02,  3.1932e-02,  1.1434e-02,\n",
      "           2.4638e-02, -1.9278e-02,  3.3447e-01,  1.1334e-01,  1.9546e-02,\n",
      "          -2.7575e-01,  5.0496e-03, -2.7683e-03,  5.1481e-03, -1.1883e-02,\n",
      "          -3.6398e-02, -7.5255e-02, -1.9376e-02,  1.1031e-01, -1.0472e-01,\n",
      "           4.1796e-02,  1.9391e-01, -1.1066e-02, -1.7442e-01,  8.5848e-03,\n",
      "           1.0153e-02, -4.1501e-02, -2.4419e-03,  2.0445e-03,  9.7902e-03,\n",
      "           1.2933e-01, -1.1450e-01,  8.2231e-02, -2.2523e-02, -3.0373e-01,\n",
      "           1.0343e-01,  3.8902e-02, -1.3350e-01,  1.5227e-01,  7.3087e-03,\n",
      "          -1.2806e-02,  3.6931e-02, -3.4073e-01, -1.9393e-01, -1.1121e-01,\n",
      "          -5.8932e-02,  7.7788e-03, -2.2143e-02,  5.9087e-02,  1.9887e-03,\n",
      "           6.5196e-02, -1.9384e-01,  1.3172e-01,  1.6935e-01,  3.0837e-03,\n",
      "          -7.7771e-02,  1.0053e-01,  2.3717e-02, -1.7053e-01,  3.0517e-02,\n",
      "           1.6077e-01, -4.2785e-02, -3.2310e-02,  2.6690e-03,  1.9632e-03,\n",
      "           1.1592e-02,  3.2835e-03, -1.1171e-01, -1.6073e-02, -1.0462e-01,\n",
      "           2.6874e-02,  8.7230e-02, -9.9795e-02, -6.0727e-05,  6.3914e-02,\n",
      "           2.4280e-03, -1.2091e-01, -2.0994e-01, -1.3902e-02,  7.7813e-03,\n",
      "           1.2956e-01, -3.6431e-02, -8.7980e-02, -3.4431e-01, -2.7144e-02,\n",
      "          -2.0259e-02, -1.7317e-03,  6.2270e-02,  5.5073e-02,  6.3938e-02,\n",
      "           1.8691e-02,  2.0834e-02, -1.1707e-01,  1.5397e-01,  4.5640e-01,\n",
      "           4.7290e-02],\n",
      "         [ 6.3787e-02, -1.6376e-01,  3.0816e-02, -1.4425e-01,  2.9936e-02,\n",
      "          -4.0784e-02,  6.4194e-03, -5.3949e-02, -5.7547e-02, -3.1257e-02,\n",
      "           1.5657e-01, -1.9032e-02, -1.3768e-01, -3.6383e-02,  9.3692e-02,\n",
      "          -3.0048e-02, -1.6614e-03, -7.2190e-03, -6.3779e-02, -8.9081e-02,\n",
      "           7.8933e-02,  2.9420e-02, -5.2841e-02,  4.1338e-02,  4.2031e-02,\n",
      "           3.7676e-02,  2.3504e-01, -1.0135e-01,  3.3336e-01, -4.5973e-02,\n",
      "           1.3058e-01, -8.7261e-02,  1.3565e-01, -3.8914e-02, -9.0997e-02,\n",
      "          -1.3077e-01, -8.7201e-02,  6.9283e-04,  3.9987e-02,  6.0623e-02,\n",
      "          -1.5416e-02,  3.8208e-02, -6.8870e-02,  7.4704e-02, -9.3580e-02,\n",
      "          -9.7708e-02, -1.3814e-01, -7.8528e-02, -7.2368e-02,  1.3577e-02,\n",
      "          -6.8871e-02, -2.8912e-02, -1.2891e-01, -4.4510e-04, -7.5380e-02,\n",
      "          -4.1036e-02, -6.0957e-02, -9.9678e-02,  6.6908e-03,  1.0327e-02,\n",
      "           3.8998e-02,  6.0386e-02, -6.7680e-03,  1.2186e-02, -1.0280e-01,\n",
      "          -9.8714e-03,  1.5375e-02, -4.3951e-01, -2.8033e-01, -1.4509e-01,\n",
      "          -1.0057e-01, -7.8050e-02,  6.1833e-02,  9.8145e-02, -8.6196e-02,\n",
      "           8.3710e-03,  3.8843e-03, -7.4849e-02, -8.5495e-02,  6.3794e-04,\n",
      "          -3.3400e-02,  1.0438e-01,  2.3189e-01, -1.0331e-01, -4.1106e-02,\n",
      "           3.8334e-02,  3.1391e-02, -4.2519e-02,  4.0804e-02,  5.6846e-02,\n",
      "           9.6407e-03,  1.6613e-01,  1.6462e-01,  2.5684e-01,  3.7289e-02,\n",
      "           9.0296e-02,  7.3047e-03, -9.6423e-02,  1.4397e-01, -6.0392e-02,\n",
      "          -8.7145e-03, -3.0156e-02,  3.0434e-02,  2.1222e-02,  4.3351e-02,\n",
      "          -2.1657e-02, -9.5463e-02,  4.9273e-02, -7.6619e-02,  1.3967e-02,\n",
      "          -1.9739e-01, -4.0639e-02, -3.3927e-03, -7.2837e-02,  1.0268e-01,\n",
      "           1.3564e-01,  1.4607e-01,  1.5956e-01,  4.7987e-02,  5.8658e-02,\n",
      "          -1.8698e-03],\n",
      "         [ 1.1127e-01, -3.0902e-02, -4.2853e-02, -3.0022e-02, -1.8481e-02,\n",
      "          -9.3700e-02, -3.4424e-02,  2.5333e-03,  2.3972e-02, -3.2269e-02,\n",
      "          -1.8231e-02,  2.2282e-02,  5.3940e-02, -5.5980e-04, -1.3209e-02,\n",
      "           4.6440e-02, -7.0722e-02, -4.9658e-02, -3.3897e-02, -1.5056e-02,\n",
      "          -3.3349e-02,  7.7586e-02, -3.1030e-02, -1.2467e-02,  2.4822e-03,\n",
      "           1.2111e-01,  1.2211e-01, -6.4218e-02,  3.0419e-01, -2.1969e-02,\n",
      "          -5.2625e-02, -5.7093e-02,  2.4083e-02, -1.2100e-02, -8.5801e-02,\n",
      "          -9.0994e-02, -1.2326e-02,  1.1256e-01, -1.7148e-02, -3.4523e-02,\n",
      "          -3.9862e-03,  4.9539e-02,  3.6653e-02, -6.7838e-02, -1.7503e-02,\n",
      "          -4.8336e-02,  7.2695e-02, -4.7984e-02,  1.1701e-02,  6.0473e-02,\n",
      "          -2.4826e-02, -9.1272e-02, -8.0561e-02, -4.1940e-03,  2.3699e-01,\n",
      "           2.8579e-02, -1.9949e-02,  3.3381e-02, -2.7399e-02, -8.0423e-03,\n",
      "          -1.5184e-01, -4.8109e-02, -5.6696e-02, -3.3408e-02, -1.7296e-02,\n",
      "          -6.9535e-02,  5.0003e-02, -3.0707e-02, -4.1940e-02, -5.0711e-02,\n",
      "           3.1522e-02, -2.5901e-02,  2.7565e-02, -2.5429e-02, -6.6305e-03,\n",
      "          -3.5490e-02, -4.0566e-02,  5.4737e-02,  2.8927e-02, -1.7251e-02,\n",
      "          -3.1583e-02, -9.8282e-03, -7.6366e-02,  1.9642e-02, -1.9606e-02,\n",
      "           8.1127e-02, -2.4309e-02, -1.0921e-03, -1.7879e-02,  5.1482e-02,\n",
      "           3.5177e-02, -2.7586e-02, -4.7016e-02,  1.1504e-02,  2.4735e-02,\n",
      "          -2.3435e-02, -4.2510e-02, -1.1482e-02, -5.7522e-02, -3.9660e-02,\n",
      "          -1.7468e-01,  2.0985e-02, -1.6527e-02, -2.3603e-02,  1.1476e-03,\n",
      "           1.9947e-02,  1.2325e-02,  6.5452e-02, -7.3957e-02, -1.7335e-01,\n",
      "          -3.8192e-03, -9.2298e-04,  2.1873e-02,  7.7223e-03,  3.3225e-02,\n",
      "          -1.9206e-02,  9.3789e-02, -3.4643e-02,  6.5035e-02, -5.9317e-02,\n",
      "           2.8345e-02],\n",
      "         [ 2.0279e-02,  9.5283e-02,  8.5869e-02, -1.1932e-02, -2.7435e-02,\n",
      "          -4.6531e-02, -9.7387e-03, -1.1262e-01, -1.4378e-01, -2.6873e-02,\n",
      "          -4.0631e-03, -1.6019e-01,  2.5757e-02,  1.4259e-02, -8.4968e-03,\n",
      "           7.7950e-03, -1.5600e-02,  1.1109e-01, -2.2348e-03,  8.7391e-02,\n",
      "          -2.8897e-02,  1.9681e-02, -2.0900e-02, -6.3021e-02, -3.8809e-02,\n",
      "           2.9024e-02, -4.2144e-02, -5.2820e-02,  1.1981e-01, -1.0239e-02,\n",
      "           7.2661e-03,  2.4140e-02,  1.8598e-02,  3.2281e-02, -1.7081e-02,\n",
      "           9.5395e-02, -6.9222e-02, -2.3085e-02,  4.1898e-02, -9.0752e-02,\n",
      "           3.2667e-02,  1.7745e-02, -7.9634e-02, -1.5449e-01,  1.1417e-01,\n",
      "           1.5617e-01,  1.4680e-01,  3.6786e-02,  5.2204e-02,  2.7170e-02,\n",
      "          -4.5256e-05, -2.6544e-03, -3.2962e-02, -2.0646e-03,  1.3826e-02,\n",
      "          -1.7081e-01, -1.4574e-02, -2.3488e-02, -4.5599e-02, -9.1519e-03,\n",
      "          -4.2788e-02,  2.4092e-02,  4.0643e-03, -3.8615e-02, -6.9306e-02,\n",
      "           1.5622e-01,  2.5019e-02,  3.0394e-02, -1.7426e-02, -1.0561e-02,\n",
      "           2.8954e-02, -5.9564e-02, -2.6421e-02,  3.1385e-02,  1.0107e-01,\n",
      "           1.1660e-01, -4.5931e-02,  1.8921e-02, -1.8100e-03, -1.1614e-01,\n",
      "           6.2943e-02, -2.1821e-01, -5.3192e-02, -2.5364e-03, -4.0884e-02,\n",
      "           3.2921e-02,  1.7827e-03, -1.4924e-02,  2.2409e-02,  1.6650e-02,\n",
      "           7.4246e-03, -5.0017e-02, -4.6683e-03, -2.3632e-02,  5.9251e-03,\n",
      "           2.0323e-01, -7.0331e-02,  3.1346e-02, -7.6131e-03,  5.4763e-02,\n",
      "           3.1888e-02, -4.6863e-02,  3.5461e-02,  4.2354e-03, -1.0742e-01,\n",
      "          -6.3476e-02,  4.5677e-02, -2.8224e-02,  6.1599e-02,  3.6851e-02,\n",
      "          -9.2390e-02,  4.7146e-03,  1.3311e-01,  7.7089e-03, -8.7610e-04,\n",
      "           8.3787e-03,  4.4988e-02,  1.7200e-01,  2.0760e-01,  2.0396e-02,\n",
      "          -1.6667e-03],\n",
      "         [ 2.5372e-02, -8.8268e-02,  1.3400e-01, -2.2416e-01,  8.4158e-02,\n",
      "          -5.8878e-02, -7.9089e-02, -1.9013e-01,  2.3467e-01,  2.8724e-02,\n",
      "          -3.5866e-02, -6.2636e-02,  6.0605e-02,  6.8518e-02,  2.3197e-02,\n",
      "          -3.4035e-02, -1.7719e-02,  3.3982e-02, -1.8469e-02,  2.3327e-02,\n",
      "          -1.2152e-01,  2.9490e-02, -3.0253e-02, -2.2239e-02,  8.9868e-03,\n",
      "           1.5455e-02, -1.1487e-02, -1.2558e-02, -1.0863e-02,  1.9061e-02,\n",
      "          -1.7258e-02, -2.5834e-02,  2.0139e-02, -1.4985e-03,  1.3536e-02,\n",
      "          -2.1020e-02,  9.5550e-02, -1.0314e-01,  4.9280e-03,  8.1924e-03,\n",
      "           1.9289e-02, -3.0775e-01,  1.7176e-02, -6.6029e-03,  2.3736e-02,\n",
      "          -1.4502e-02,  8.5701e-03,  5.8543e-02, -2.8631e-02, -4.5965e-03,\n",
      "          -1.4329e-01,  5.7613e-02,  1.0039e-02, -1.8828e-02, -3.0512e-02,\n",
      "          -5.7447e-02,  6.8682e-02,  1.0008e-01, -5.3633e-02, -1.4663e-02,\n",
      "           7.2284e-03, -7.0824e-03, -2.2979e-02,  4.0873e-02, -1.8001e-03,\n",
      "           3.7122e-02, -6.5899e-02,  2.5943e-02, -1.0063e-01, -1.3579e-02,\n",
      "          -3.7565e-02,  1.5248e-02,  1.4029e-02,  2.7709e-02,  6.6629e-04,\n",
      "          -3.0230e-02,  8.1997e-03,  5.2671e-02, -4.6203e-02, -2.7773e-02,\n",
      "          -4.9196e-03,  5.3079e-02,  2.5258e-02, -9.9306e-02, -1.3924e-01,\n",
      "          -7.0178e-02, -2.7937e-02,  2.3781e-02, -1.1756e-01, -6.6815e-02,\n",
      "          -2.0838e-02,  1.4996e-02,  2.2328e-02, -1.8624e-02, -9.3023e-02,\n",
      "           6.2338e-02, -5.9598e-02,  3.0929e-02,  3.1253e-02, -1.6537e-02,\n",
      "          -9.4714e-03, -6.8855e-04,  1.0447e-01,  5.9245e-02, -1.3831e-02,\n",
      "           9.3714e-03,  2.0621e-03,  5.1684e-02, -1.3236e-01,  9.8448e-03,\n",
      "          -9.0581e-02, -4.8302e-03, -2.6287e-02,  4.1839e-02, -3.2587e-02,\n",
      "           2.0013e-02,  1.2582e-02,  4.8887e-02, -3.5517e-01, -1.6232e-01,\n",
      "          -2.6736e-02],\n",
      "         [-2.4169e-03, -3.6385e-02,  1.1083e-02, -3.6688e-02, -4.0055e-02,\n",
      "          -9.4281e-02, -2.4441e-02,  4.8766e-03, -3.5801e-02,  3.6648e-02,\n",
      "           6.6596e-02,  6.2376e-02,  2.6852e-01,  5.3106e-02, -9.9378e-02,\n",
      "           5.2981e-02,  5.6627e-02, -9.1314e-03, -2.6203e-01, -4.6900e-02,\n",
      "          -1.0373e-01,  8.8588e-02, -4.7880e-02,  7.5695e-03,  2.0135e-02,\n",
      "          -2.0289e-01, -2.7667e-01, -8.5509e-02,  1.0559e-01, -6.5337e-02,\n",
      "           1.7845e-01,  1.2344e-01,  7.0363e-02, -7.1583e-02,  3.5301e-02,\n",
      "          -4.3843e-02, -3.9939e-02, -3.2758e-02, -2.0703e-02, -2.0648e-03,\n",
      "          -6.9580e-03, -9.2295e-02, -4.9032e-03,  5.6060e-02,  1.2208e-01,\n",
      "           1.5742e-02,  2.4431e-02, -5.6354e-02, -8.3702e-02, -4.1810e-02,\n",
      "          -1.4442e-01,  1.1117e-01,  2.7876e-02, -6.1413e-03,  9.3771e-02,\n",
      "           3.3511e-02,  3.4776e-02, -1.1174e-02, -4.0809e-02, -9.3390e-03,\n",
      "           3.0576e-02, -1.6938e-03,  5.1910e-02,  8.3208e-04, -6.6001e-02,\n",
      "           5.5766e-02,  1.2691e-01,  5.5943e-02, -3.6922e-01, -7.2966e-02,\n",
      "          -1.7116e-01,  1.9643e-02, -1.1568e-01, -8.3043e-02, -1.2760e-01,\n",
      "           8.4946e-02, -7.0733e-02, -2.7320e-02, -6.1186e-03,  1.9488e-02,\n",
      "           6.3765e-02,  5.8321e-02, -5.2578e-02, -6.9702e-02,  7.4696e-02,\n",
      "           2.5960e-02,  2.7990e-02, -2.0812e-02,  8.9889e-03, -2.4779e-02,\n",
      "           1.2874e-01,  2.6557e-02,  1.1621e-01, -2.7580e-02, -1.2080e-01,\n",
      "           8.8498e-02, -1.7262e-02, -1.4314e-02,  1.3770e-01, -2.7215e-01,\n",
      "           9.7232e-03,  2.0084e-02,  3.9186e-02, -1.5324e-02, -5.8298e-02,\n",
      "          -4.7486e-03,  6.4574e-02, -1.7292e-01, -1.8255e-02,  1.9782e-01,\n",
      "          -1.1995e-01, -3.4823e-02,  5.6560e-02, -7.5011e-02, -7.0082e-02,\n",
      "           8.3788e-02,  9.6704e-03,  3.2201e-02,  1.1985e-01,  1.4581e-02,\n",
      "          -2.2037e-02],\n",
      "         [-1.8450e-02,  2.1092e-02,  3.3380e-02,  5.1129e-03,  1.5519e-02,\n",
      "          -9.9155e-02,  5.3609e-03, -9.4971e-02, -3.1567e-02,  1.2508e-02,\n",
      "          -1.8189e-02,  1.0164e-02, -7.0579e-02, -1.4037e-02,  1.6940e-02,\n",
      "           1.6591e-02, -6.4536e-03, -5.4862e-03,  2.2531e-02,  5.0894e-02,\n",
      "           1.7569e-03, -9.5657e-02, -1.0811e-03,  3.3091e-02, -2.3981e-03,\n",
      "          -3.2366e-02, -2.7981e-02, -1.4089e-01,  5.2410e-02,  7.2453e-03,\n",
      "           1.4316e-02, -8.3672e-03,  3.0553e-02, -1.2810e-02,  4.7923e-02,\n",
      "          -2.3704e-02,  9.8488e-02,  1.0965e-02,  3.4046e-02, -9.6437e-03,\n",
      "           1.3695e-03,  1.5347e-02, -3.0674e-02, -7.8838e-02,  5.1164e-03,\n",
      "          -1.9241e-03,  4.3817e-02,  2.8257e-02, -3.4723e-02,  5.1459e-02,\n",
      "           3.7109e-02,  4.1877e-02, -4.8356e-02,  5.2266e-02, -1.1949e-02,\n",
      "           6.2131e-02,  1.8463e-03,  6.1113e-02,  1.1410e-02,  1.5275e-02,\n",
      "           7.3043e-03, -2.9496e-02,  4.3007e-02, -8.3440e-02, -4.2152e-02,\n",
      "          -1.7602e-03,  1.4781e-01, -1.0470e-02, -1.0914e-01, -3.5373e-02,\n",
      "          -3.4771e-02,  4.9697e-02, -4.5876e-02, -1.8213e-02,  4.2146e-02,\n",
      "          -8.8501e-03,  2.4416e-02, -9.8699e-03, -1.6091e-02, -2.2915e-02,\n",
      "           1.6031e-02, -5.8583e-02, -7.9318e-03, -8.4679e-02, -4.2435e-03,\n",
      "           1.3281e-01,  8.8884e-03,  2.6264e-02, -3.7780e-03,  2.4981e-02,\n",
      "           6.1582e-04,  9.6548e-03, -1.4228e-01,  8.2060e-03,  4.7960e-03,\n",
      "           8.6811e-02, -4.8220e-02,  3.9730e-02,  7.2452e-02, -1.2456e-02,\n",
      "           5.4123e-02,  3.2564e-03,  3.9199e-02,  5.6306e-03,  1.2676e-02,\n",
      "          -2.4914e-02, -2.8182e-02, -3.1725e-03, -2.0250e-02,  6.6329e-02,\n",
      "          -9.3606e-02,  3.0749e-02, -3.0250e-02, -2.3184e-02,  1.6072e-02,\n",
      "           1.2952e-01, -4.1668e-02, -4.8434e-02, -5.5234e-03,  3.0152e-02,\n",
      "           1.0588e-02],\n",
      "         [ 2.1459e-01, -5.9678e-02,  6.9071e-03, -1.2385e-02,  1.0480e-02,\n",
      "          -6.4953e-02,  4.2358e-02, -5.2026e-02, -2.7912e-02,  7.6816e-02,\n",
      "           7.0124e-03,  1.1190e-02,  1.4535e-01,  5.6816e-02, -2.9301e-03,\n",
      "           1.6812e-02, -2.0330e-02,  6.8894e-02, -7.1738e-02,  3.7378e-02,\n",
      "          -3.9019e-02,  1.6576e-01,  3.6095e-02, -4.9423e-02, -2.9831e-02,\n",
      "           8.3598e-04,  2.6017e-02, -8.8311e-03, -1.6829e-01, -1.0577e-02,\n",
      "           2.5553e-01,  5.0790e-02,  3.3757e-03, -1.7088e-02, -2.4428e-02,\n",
      "          -5.2291e-03, -6.9695e-03, -3.0000e-02,  1.5901e-02,  3.0495e-03,\n",
      "          -5.0125e-02,  8.6627e-02,  2.1223e-02,  3.1946e-02, -5.2298e-02,\n",
      "           1.8283e-02,  3.0794e-02,  3.4378e-04, -6.0540e-02, -1.8264e-01,\n",
      "          -2.0983e-02,  4.6077e-02,  3.6940e-02, -8.3177e-05, -3.8875e-02,\n",
      "          -1.9208e-02,  7.2928e-03,  1.7207e-02,  3.9042e-02,  4.4969e-02,\n",
      "           6.7246e-02,  3.4470e-02,  2.8790e-02, -1.6965e-03, -7.3255e-03,\n",
      "           3.1380e-02,  2.9378e-02, -1.0268e-01, -6.4370e-02, -2.2391e-02,\n",
      "          -6.6205e-02,  3.7072e-02,  1.2139e-01, -8.2429e-03,  1.9559e-02,\n",
      "          -3.4529e-03,  5.7377e-03, -1.9818e-02,  1.6393e-03,  7.4968e-02,\n",
      "           1.4149e-02,  3.0183e-02, -9.9359e-03,  3.2595e-02,  8.6053e-02,\n",
      "          -1.2292e-03, -5.9824e-02, -3.8753e-02,  2.3317e-02, -4.3407e-02,\n",
      "           5.4211e-02, -4.5312e-02, -2.0807e-02,  8.4739e-03,  6.1665e-02,\n",
      "           5.1078e-02,  1.5467e-03,  3.5399e-02, -4.3338e-03, -1.4219e-02,\n",
      "           1.3679e-02, -6.4130e-03, -1.0927e-01, -1.0982e-02,  8.2357e-02,\n",
      "           2.4369e-02, -6.4468e-02, -8.5840e-02, -6.4285e-02, -1.1449e-01,\n",
      "          -4.1446e-02, -8.8346e-02, -1.5155e-02, -3.0005e-02, -1.5476e-01,\n",
      "          -1.6239e-03,  2.2461e-03,  1.8150e-02, -3.2798e-02,  2.3234e-02,\n",
      "           4.6885e-02]]])), ('gat_net.0.scoring_fn_source', tensor([[[-1.9268e-02,  2.2934e-02, -1.2114e-01, -3.3607e-02, -2.4308e-02,\n",
      "          -1.4807e-02,  9.6777e-03,  2.7985e-01, -2.0793e-01,  3.2110e-02,\n",
      "          -7.8460e-02, -3.7850e-02,  1.7870e-01, -2.1342e-02, -8.4578e-03,\n",
      "          -1.2011e-02,  3.5345e-02, -1.4372e-01,  1.2948e-03,  1.1293e-01,\n",
      "          -3.0739e-02,  7.7111e-02, -4.8474e-02,  3.2915e-03, -7.8324e-02,\n",
      "           6.4918e-02, -3.4418e-03,  1.2766e-03,  7.2909e-02, -1.0872e-02,\n",
      "           1.1200e-01, -8.8780e-02,  1.0027e-01, -1.9175e-01, -3.5753e-02,\n",
      "           4.2892e-02, -1.4664e-01, -9.1153e-03, -1.8725e-01, -5.2952e-02,\n",
      "           9.5879e-02, -2.1249e-02,  3.1631e-03, -5.9102e-02,  1.0164e-02,\n",
      "           2.0037e-01,  2.4476e-01,  1.3386e-01,  6.1405e-02,  3.6655e-03,\n",
      "           6.2318e-02,  1.9641e-01, -1.5976e-01,  2.3114e-01, -1.2036e-01,\n",
      "          -1.1703e-01,  5.7246e-03, -1.2922e-02,  5.3311e-03,  2.2381e-01,\n",
      "           4.7590e-02, -4.7054e-02, -1.0086e-01,  2.6110e-03, -9.8438e-02,\n",
      "          -3.9196e-02,  2.0261e-01, -2.7672e-02, -6.7891e-03, -1.7572e-03,\n",
      "           4.0658e-02, -8.1974e-02, -6.5063e-03, -1.0660e-01, -1.7477e-02,\n",
      "          -1.1512e-01,  3.7638e-01, -4.2490e-02, -7.3622e-02, -8.6383e-03,\n",
      "           2.4024e-02,  2.7558e-01, -6.5304e-02,  9.0673e-03, -8.4391e-02,\n",
      "          -7.0766e-02, -2.0470e-01, -7.6671e-02, -1.1164e-01,  2.5045e-01,\n",
      "          -2.6621e-02, -1.1621e-01,  5.8009e-02, -5.0368e-03, -9.1153e-04,\n",
      "          -9.5797e-03,  2.7363e-04,  1.5223e-01,  2.1980e-01, -3.6046e-02,\n",
      "          -9.6480e-03, -7.4130e-03,  1.3810e-02,  2.3547e-01, -5.7847e-02,\n",
      "          -6.6084e-02, -2.8533e-02, -1.8058e-02,  2.7798e-02, -1.3645e-01,\n",
      "          -9.2138e-02, -9.0208e-02, -1.4380e-02, -7.5739e-02, -1.0956e-01,\n",
      "          -6.1173e-02,  1.7910e-01,  9.0542e-03, -1.2073e-01,  1.4707e-01,\n",
      "          -1.1385e-01],\n",
      "         [ 1.1072e-01, -1.7297e-02, -4.6092e-02,  2.2784e-01, -1.4304e-01,\n",
      "           1.1885e-02,  2.1145e-02,  7.0289e-03, -1.2470e-02, -2.9467e-02,\n",
      "          -8.0619e-02,  2.2748e-02, -2.8212e-02,  6.8848e-03, -5.6504e-03,\n",
      "           1.7780e-01, -8.8954e-02, -1.5374e-01, -1.0879e-01, -5.3625e-02,\n",
      "           8.7540e-02,  1.3595e-01,  2.4669e-01, -1.3419e-02,  1.4453e-02,\n",
      "           9.2288e-02,  6.5820e-02, -8.5964e-02,  8.3411e-02, -2.2374e-02,\n",
      "           1.4494e-01,  6.1187e-03,  1.4619e-01, -8.1629e-02, -1.0213e-02,\n",
      "           3.8869e-03,  2.2658e-01, -5.3899e-02, -1.1800e-01, -2.3459e-02,\n",
      "           3.3890e-02,  1.9324e-02, -2.0695e-01, -4.9640e-02, -6.7548e-02,\n",
      "          -2.1567e-02,  4.2451e-02, -2.2068e-02, -8.7130e-02,  2.2187e-02,\n",
      "           1.5687e-01,  3.0050e-03,  1.8097e-01,  3.6619e-02,  9.2044e-03,\n",
      "          -1.8195e-01, -7.0536e-02, -6.0090e-02, -6.2607e-02,  2.0600e-01,\n",
      "          -1.7235e-02,  7.6110e-03,  1.2345e-02, -7.1529e-02, -1.1002e-01,\n",
      "          -1.3494e-03, -2.8474e-02, -3.9724e-02,  1.1933e-02,  2.5305e-02,\n",
      "           1.1114e-02,  1.5746e-02, -1.8195e-01, -1.6903e-01, -2.5247e-02,\n",
      "           4.3877e-02, -5.1396e-02, -1.0558e-01,  2.0906e-01,  4.5769e-02,\n",
      "           3.8146e-01,  2.0288e-02, -2.1954e-01, -1.7753e-01,  3.9306e-02,\n",
      "          -7.4717e-02, -5.4083e-02,  2.8493e-02,  7.1302e-02, -1.0584e-01,\n",
      "           7.8492e-03, -6.2564e-02,  8.1398e-03, -6.3926e-02,  7.8285e-03,\n",
      "           5.1931e-02,  2.6139e-02, -2.9608e-02,  2.0276e-01,  3.3080e-03,\n",
      "          -4.9433e-02,  2.6650e-02,  2.4455e-03, -1.6320e-01, -4.7129e-03,\n",
      "          -1.2662e-01, -3.4827e-02,  1.2641e-02,  4.4705e-02, -6.7523e-03,\n",
      "           2.5564e-01, -1.1825e-01,  2.0490e-01, -4.8754e-03, -2.6314e-02,\n",
      "           2.2064e-01, -2.4704e-02,  4.1291e-02,  6.7131e-02, -7.4326e-02,\n",
      "           1.8337e-01],\n",
      "         [-2.9378e-02,  1.9685e-01,  3.4920e-02, -5.3869e-02, -1.3520e-02,\n",
      "           3.3944e-03, -5.3413e-02,  3.7468e-03,  3.0514e-02,  1.7623e-01,\n",
      "          -2.1005e-02, -1.8792e-02,  2.8639e-01, -6.3403e-02,  1.5590e-02,\n",
      "          -3.4685e-02,  3.5546e-02,  1.4786e-02, -2.1160e-01, -5.3220e-02,\n",
      "          -1.0472e-01,  4.7280e-02, -2.3296e-02, -1.2063e-01, -8.7624e-02,\n",
      "           1.0425e-01,  5.6813e-03,  6.3091e-02,  6.2567e-02, -5.3844e-02,\n",
      "           7.0727e-02, -3.1924e-02,  1.6748e-01,  6.0108e-03,  2.2252e-01,\n",
      "           7.2750e-03,  3.2358e-01, -4.1802e-02,  2.1200e-01, -1.3692e-01,\n",
      "           2.4923e-03, -7.9400e-02, -1.0783e-01,  2.0951e-01,  5.1085e-02,\n",
      "          -2.0292e-02,  5.2997e-03, -1.0703e-01, -3.4915e-02, -3.9750e-02,\n",
      "          -5.8076e-02, -9.1899e-03, -6.9692e-03, -8.6763e-02,  4.9457e-02,\n",
      "          -3.3873e-02, -8.6101e-02, -7.4624e-02,  1.4675e-02, -9.9324e-02,\n",
      "           2.3702e-01, -3.7203e-02,  1.9769e-02, -4.5607e-02,  4.3834e-02,\n",
      "          -3.1294e-02, -1.4160e-02,  5.1709e-02, -1.0132e-01,  1.9866e-02,\n",
      "          -1.5120e-01,  4.1869e-02, -1.4125e-01,  2.5019e-02, -5.4724e-02,\n",
      "          -8.0902e-02, -3.3281e-02, -5.5977e-02, -2.8733e-02,  3.1880e-01,\n",
      "          -2.1948e-03, -8.0508e-03,  8.1469e-03, -7.8005e-03, -1.7516e-02,\n",
      "           1.2402e-01, -7.3225e-02,  2.5463e-01,  2.0802e-01, -9.9877e-02,\n",
      "          -1.2615e-02, -8.5524e-02, -6.5487e-03,  1.5052e-02, -1.8033e-01,\n",
      "          -4.1639e-02, -3.2154e-03,  1.5360e-02, -7.0461e-02, -2.2140e-02,\n",
      "           1.8954e-01, -7.1972e-03, -2.2336e-01,  2.5081e-01,  1.2943e-02,\n",
      "           3.2542e-01,  6.8722e-03, -6.2819e-02,  2.2052e-01,  8.7649e-03,\n",
      "          -1.9517e-01, -6.2846e-02, -1.5802e-02,  8.5021e-02, -1.2679e-01,\n",
      "          -6.1654e-02,  1.1445e-01,  3.1175e-01,  5.3122e-03, -1.3776e-01,\n",
      "          -6.9701e-02],\n",
      "         [ 1.9470e-01, -9.7122e-02, -3.2289e-01, -2.6883e-02, -1.8880e-01,\n",
      "          -4.6186e-02, -4.7611e-02, -2.4383e-02, -1.2270e-02,  2.6589e-03,\n",
      "          -2.0855e-01,  1.4363e-01, -3.2966e-02, -9.1831e-02, -4.9679e-03,\n",
      "          -2.1845e-02,  1.6359e-02, -3.5900e-03,  2.7119e-03,  2.0507e-01,\n",
      "          -1.4817e-02,  1.2160e-01, -1.0563e-01, -8.5821e-03, -1.2749e-02,\n",
      "          -1.5651e-01,  1.0925e-01,  6.7924e-03, -6.3326e-04, -3.5101e-03,\n",
      "           2.2012e-01, -8.2552e-03,  2.1899e-01,  4.6810e-03, -1.4643e-03,\n",
      "           1.3434e-02, -4.8153e-02, -3.2711e-02, -1.7952e-02,  1.5415e-03,\n",
      "          -2.2403e-02, -1.2929e-02, -1.6301e-02,  5.3020e-03,  6.0527e-03,\n",
      "          -1.6193e-01, -1.6788e-02, -3.4359e-02, -1.5525e-02,  5.4418e-03,\n",
      "          -2.0336e-01, -2.6358e-02,  1.3501e-01, -1.1159e-02, -3.5350e-03,\n",
      "           1.8886e-01, -1.1450e-01, -1.2558e-02, -8.2008e-02, -2.0131e-03,\n",
      "           1.2651e-01,  2.2590e-03, -3.3541e-02, -2.2697e-02, -5.2763e-02,\n",
      "          -7.1367e-02,  4.3516e-03, -1.3117e-02, -2.4034e-03,  4.4388e-03,\n",
      "          -1.4442e-02, -1.6033e-02, -4.0308e-04, -9.1073e-02, -1.0267e-01,\n",
      "          -2.8357e-02, -9.0501e-03, -2.1666e-01, -5.8433e-02,  1.8536e-02,\n",
      "          -5.6954e-02,  6.7612e-02,  1.5341e-01, -9.5583e-03, -3.5953e-02,\n",
      "           1.4837e-01, -7.4726e-02, -1.7409e-01, -1.8072e-01, -5.9153e-03,\n",
      "           5.7174e-03,  8.9905e-05, -1.3601e-01,  1.6365e-01, -2.2781e-02,\n",
      "          -4.0362e-02, -3.5810e-02,  1.4944e-01, -6.2842e-02, -4.8831e-02,\n",
      "          -1.1703e-01, -1.3105e-03, -4.5320e-02, -2.2898e-02,  1.3231e-01,\n",
      "           1.7429e-01, -8.1494e-02, -2.0521e-01, -1.1717e-03,  2.3109e-02,\n",
      "           7.4198e-02, -4.2758e-03, -1.0839e-02, -1.9279e-02, -7.5676e-03,\n",
      "           1.6560e-02,  3.4469e-01,  6.0214e-02,  5.6736e-02,  5.5946e-02,\n",
      "          -6.1230e-03],\n",
      "         [ 5.6037e-02, -4.1271e-03,  1.2014e-03,  1.2465e-01, -1.3546e-01,\n",
      "          -2.0692e-01,  8.2847e-03,  3.1901e-02,  4.4108e-03,  5.7639e-03,\n",
      "           1.3502e-01, -2.9181e-02,  1.0351e-02, -2.0226e-02, -9.1218e-02,\n",
      "          -1.2099e-01, -2.6717e-01, -1.2061e-01, -2.5507e-02, -2.9600e-02,\n",
      "           3.0879e-02,  4.6860e-02, -1.3638e-01, -2.8997e-02,  5.8929e-03,\n",
      "           8.4492e-02, -1.8197e-02, -4.6910e-02,  5.1856e-02,  5.7438e-02,\n",
      "           2.6758e-02, -4.8000e-02, -4.8556e-03,  9.9020e-03,  2.6651e-02,\n",
      "           1.1694e-02, -1.3839e-02,  4.1608e-03, -2.5977e-02, -6.0000e-02,\n",
      "          -1.3410e-01, -1.5923e-02,  2.7859e-02, -1.2640e-01, -3.4826e-02,\n",
      "          -4.9144e-02, -1.1475e-01,  6.1690e-04, -1.4579e-02, -3.8246e-02,\n",
      "           1.0357e-01, -6.3734e-03,  1.8439e-01,  1.5941e-03, -1.8644e-03,\n",
      "           1.4287e-01,  1.0193e-01,  3.5402e-03, -4.5293e-02, -2.0040e-03,\n",
      "           2.1897e-01,  2.8209e-01, -1.8209e-01, -2.9216e-03, -5.6942e-02,\n",
      "          -2.7340e-02, -1.5502e-01, -4.3658e-02,  3.9299e-02,  5.2499e-03,\n",
      "          -1.3193e-01,  6.6210e-03,  3.6586e-03, -3.8147e-02, -1.5929e-01,\n",
      "           1.3101e-02, -1.6911e-02, -2.3039e-02, -1.2284e-02, -1.1161e-01,\n",
      "          -2.6297e-02,  1.7447e-01, -6.0264e-02,  1.0722e-02, -1.2474e-02,\n",
      "           7.5714e-04, -1.8497e-01, -1.4130e-02,  1.5864e-01, -9.6620e-02,\n",
      "           3.5202e-03,  1.2167e-03,  2.3522e-01, -7.3435e-02, -1.7378e-01,\n",
      "          -1.4382e-01, -5.0453e-02, -4.0308e-02,  2.0210e-01, -3.6666e-02,\n",
      "           1.5147e-01, -3.9321e-02, -1.9585e-02,  2.5604e-02, -1.4467e-01,\n",
      "          -2.4915e-02,  2.4564e-01, -5.1978e-03,  1.5480e-01, -6.9138e-02,\n",
      "           9.6148e-02, -3.8032e-02,  1.6656e-01, -3.8674e-02,  3.2167e-03,\n",
      "          -6.6295e-02,  2.1677e-01,  1.0759e-01,  1.2890e-03,  1.3038e-01,\n",
      "          -2.2926e-02],\n",
      "         [ 1.6812e-01, -2.3395e-02,  2.1902e-02,  1.2103e-02, -4.6034e-02,\n",
      "           1.8811e-01,  1.6132e-01, -9.1409e-03, -1.0064e-01,  2.4008e-02,\n",
      "          -5.7300e-02, -2.5628e-02,  1.6272e-01, -8.2550e-02,  4.5448e-02,\n",
      "          -1.8666e-02, -2.0467e-01,  1.9255e-03, -4.4361e-02,  6.0838e-02,\n",
      "          -1.0521e-01,  1.2778e-01, -8.5422e-02, -1.4048e-02, -5.4695e-02,\n",
      "           3.3844e-02,  7.4331e-02,  1.6540e-01, -2.7522e-01, -4.7224e-02,\n",
      "          -1.4097e-01, -4.3386e-02,  1.8797e-01,  1.5293e-02, -8.7317e-02,\n",
      "          -1.9031e-02, -5.9961e-02, -7.7174e-03,  1.7369e-01,  3.0336e-01,\n",
      "          -2.2915e-01, -6.6170e-02,  2.0015e-03, -9.2146e-03,  1.4015e-02,\n",
      "          -8.4947e-02,  8.2576e-03,  1.2124e-01, -9.9060e-02, -1.6946e-02,\n",
      "           1.5326e-01, -1.2342e-01, -1.0919e-01,  1.1466e-01, -2.7210e-02,\n",
      "          -7.8628e-03, -8.5421e-03, -2.6495e-02, -6.5236e-02, -4.9431e-02,\n",
      "          -5.1048e-02, -3.2841e-02, -8.9370e-02,  1.8276e-01,  5.1830e-03,\n",
      "           5.6522e-03, -9.9628e-03,  1.5288e-01,  1.8505e-01, -1.1884e-02,\n",
      "          -4.8316e-02, -1.0915e-01, -1.2440e-02,  2.2071e-01,  1.4298e-01,\n",
      "           9.9014e-03, -3.5811e-03, -6.6924e-02, -1.3284e-02, -2.8657e-02,\n",
      "          -3.5812e-03,  1.0822e-01,  3.0197e-03,  9.0691e-03, -8.7645e-02,\n",
      "           2.3542e-01,  6.8444e-03, -8.3668e-02, -7.9584e-03, -8.7582e-02,\n",
      "           1.0613e-02,  2.0643e-02, -1.0550e-01, -1.2969e-01,  4.2902e-02,\n",
      "          -1.1851e-02,  6.6809e-03,  2.5377e-01, -1.3471e-01,  1.4112e-01,\n",
      "           2.5607e-03, -4.5459e-02, -6.9730e-02, -5.4169e-03,  8.1195e-03,\n",
      "           2.0557e-03, -4.3207e-02,  3.9438e-04,  1.4939e-01, -1.7563e-02,\n",
      "           2.3816e-01,  2.2722e-02, -3.8567e-02, -1.5906e-03, -4.1439e-02,\n",
      "          -8.7874e-02,  1.6561e-01, -2.8563e-03,  1.1476e-01, -2.4569e-02,\n",
      "          -1.9555e-02],\n",
      "         [ 1.7856e-02, -5.8199e-02,  5.4611e-02, -5.4088e-02, -6.7853e-02,\n",
      "           1.9679e-03,  5.4739e-02,  1.0932e-02,  6.2114e-03, -3.8404e-02,\n",
      "          -1.2233e-01, -1.0646e-01,  6.3413e-02,  2.9369e-02, -1.2655e-01,\n",
      "           1.2978e-01, -1.0473e-01, -3.7260e-02,  2.6793e-03, -7.7614e-02,\n",
      "          -9.0011e-02, -4.5938e-03, -5.9794e-03, -3.6906e-03,  3.2029e-02,\n",
      "           7.7819e-02,  4.9512e-02, -1.3476e-02,  1.7314e-01,  1.0248e-02,\n",
      "          -1.0140e-01, -9.6718e-03,  1.1801e-01, -4.6284e-02, -7.3229e-02,\n",
      "           1.2885e-02,  3.4292e-02,  6.4306e-02,  8.8671e-02, -5.1290e-02,\n",
      "          -2.2827e-01, -5.1926e-02,  1.7249e-02,  7.8817e-03, -1.5216e-02,\n",
      "          -3.5995e-04, -2.3279e-02, -7.0177e-02,  4.7639e-03, -6.2958e-02,\n",
      "          -4.8347e-02, -4.3164e-02,  3.0014e-02,  1.4872e-01, -3.5093e-03,\n",
      "          -1.0952e-01,  1.3265e-01, -2.6784e-01,  3.0204e-04, -6.6742e-02,\n",
      "           6.9496e-02, -1.2114e-01,  3.5501e-03,  1.5732e-01,  8.1777e-03,\n",
      "          -1.6189e-01, -1.2560e-01,  2.4954e-02,  2.1847e-01, -3.9917e-02,\n",
      "          -1.0597e-01,  1.1494e-02,  2.8165e-02,  1.4547e-01, -2.0414e-02,\n",
      "          -1.3274e-01,  7.7373e-04, -3.3326e-02,  8.1042e-03,  4.2387e-02,\n",
      "          -4.0113e-02, -3.4164e-03,  2.9792e-02, -8.9768e-02, -1.4598e-01,\n",
      "          -1.1693e-01,  3.6103e-03, -2.2866e-02,  1.7351e-01, -1.2193e-01,\n",
      "          -1.6766e-01,  4.2217e-05,  2.0430e-01,  3.2320e-02, -2.0289e-01,\n",
      "          -3.4940e-02, -1.0329e-01, -1.0196e-01,  6.8879e-02, -1.4419e-02,\n",
      "          -1.7025e-01, -7.1428e-02, -4.4245e-02,  8.9184e-02, -3.2023e-02,\n",
      "          -3.9573e-03, -1.6027e-02, -3.4800e-02, -3.7928e-02, -1.3624e-02,\n",
      "           2.9432e-02, -2.7991e-02,  1.2911e-02,  1.8481e-02, -6.1432e-02,\n",
      "           1.4992e-01,  7.9050e-02,  2.8157e-01,  2.6057e-01,  1.1511e-01,\n",
      "          -8.0459e-02],\n",
      "         [ 8.3233e-03, -3.3031e-02,  6.2150e-03, -1.6723e-02, -1.2927e-02,\n",
      "          -2.5877e-01, -1.6312e-01,  1.8761e-01, -1.5625e-01, -1.9564e-02,\n",
      "           1.6375e-03, -2.7853e-03,  1.4135e-02, -1.2170e-02, -8.1499e-03,\n",
      "          -9.7658e-02, -4.9006e-02,  1.0304e-02,  2.3631e-01,  2.3131e-01,\n",
      "           5.1894e-02,  4.4473e-02, -2.4059e-02,  2.0212e-01,  2.5070e-02,\n",
      "          -3.3283e-02,  2.4491e-02,  1.9316e-01,  1.0443e-02, -5.6715e-02,\n",
      "          -2.5382e-02, -2.2471e-02,  8.3909e-02, -7.5388e-02, -1.2130e-01,\n",
      "           2.1977e-02,  1.1186e-02,  1.3552e-01,  5.8836e-03, -3.3818e-03,\n",
      "          -1.6691e-01,  1.9995e-02, -2.1935e-02, -2.9553e-02, -1.7130e-01,\n",
      "           2.9271e-01, -1.1602e-02, -1.7364e-02, -3.9772e-02,  5.1059e-02,\n",
      "          -5.0830e-02,  4.5711e-02, -7.6728e-02, -3.8586e-02, -1.0153e-02,\n",
      "          -9.4278e-03, -3.1909e-02, -1.0996e-02,  8.0869e-03, -7.4131e-03,\n",
      "           8.6264e-02,  5.3607e-03,  9.5544e-04, -1.7659e-01, -5.7070e-02,\n",
      "          -5.4787e-02,  2.7813e-01, -2.7040e-02,  1.4186e-02,  3.4955e-02,\n",
      "          -1.3321e-02, -2.6125e-02, -5.1060e-02, -6.2949e-03, -4.2149e-03,\n",
      "          -2.2898e-02, -3.5706e-02, -2.0148e-02, -7.6198e-03, -8.3580e-02,\n",
      "          -5.6492e-02, -1.0680e-01,  8.0697e-02, -5.8993e-02, -1.2228e-01,\n",
      "           1.7047e-01, -9.0889e-03,  8.2441e-02,  1.0286e-02, -3.3888e-03,\n",
      "          -2.0649e-02,  1.0028e-01,  3.7290e-02, -1.4813e-02,  2.7331e-02,\n",
      "           1.5032e-01, -1.4147e-01, -1.1390e-01, -8.4528e-02, -1.7283e-02,\n",
      "          -2.9744e-02, -8.1257e-03, -5.7469e-02, -8.1295e-03, -2.5862e-02,\n",
      "           4.2399e-03, -1.4435e-02,  6.5288e-03,  7.0510e-03, -1.4257e-02,\n",
      "          -1.0021e-01, -7.6167e-03, -1.1384e-02, -3.3668e-02,  2.0173e-01,\n",
      "           1.1537e-01,  2.0830e-01,  1.4544e-01,  1.9953e-01, -4.0000e-02,\n",
      "           2.1203e-02]]])), ('gat_net.0.linear_proj.weight', tensor([[ 0.1896, -0.1550,  0.4376,  ..., -0.6113, -0.0396,  0.0839],\n",
      "        [-0.1270,  0.4354, -0.8233,  ..., -0.0778,  0.4245, -0.6555],\n",
      "        [-0.2329,  0.1116,  0.2512,  ..., -0.6231, -0.1202, -0.1102],\n",
      "        ...,\n",
      "        [-0.2862, -0.6668, -0.6405,  ...,  0.3365, -1.1622, -0.0740],\n",
      "        [ 0.5182,  0.5139,  1.0480,  ...,  0.0961, -0.2902, -0.4000],\n",
      "        [ 1.2718,  1.3327,  1.7794,  ...,  0.0034, -0.0526,  0.0538]]))])}\n"
     ]
    }
   ],
   "source": [
    "print(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c5f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = dict()\n",
    "for key, value in model_state[\"state_dict\"].items():\n",
    "    param_dict[key] = value.cpu().detach().numpy().tolist()\n",
    "text_file = open(\"./gat_ppi_model.json\", \"w\")\n",
    "text_file.write(json.dumps(param_dict, indent=4))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecfe49a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "968"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(param_dict[\"gat_net.0.linear_proj.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0015e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: 8 (num_heads) x out_dim (121)\n",
    "model_file = open(\"./gat_ppi_model.txt\", \"w\")\n",
    "source_params = param_dict[\"gat_net.0.scoring_fn_source\"][0]\n",
    "for row in source_params:\n",
    "    line = \"\"\n",
    "    for elem in row:\n",
    "        if line == \"\":\n",
    "            line = str(elem)\n",
    "        else:\n",
    "            line += \" \" + str(elem)\n",
    "    line += \"\\n\"\n",
    "    model_file.write(line)\n",
    "# target: 8 (num_heads) x out_dim (121)\n",
    "target_params = param_dict[\"gat_net.0.scoring_fn_target\"][0]\n",
    "for row in target_params:\n",
    "    line = \"\"\n",
    "    for elem in row:\n",
    "        if line == \"\":\n",
    "            line = str(elem)\n",
    "        else:\n",
    "            line += \" \" + str(elem)\n",
    "    line += \"\\n\"\n",
    "    model_file.write(line)\n",
    "    \n",
    "# linear_proj: 968 (121 output_dim * 8 num_heads) x 50 (input_dim)\n",
    "linear_proj = param_dict[\"gat_net.0.linear_proj.weight\"]\n",
    "for row in linear_proj:\n",
    "    line = \"\"\n",
    "    for elem in row:\n",
    "        if line == \"\":\n",
    "            line = str(elem)\n",
    "        else:\n",
    "            line += \" \" + str(elem)\n",
    "    line += \"\\n\"\n",
    "    model_file.write(line)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6d572",
   "metadata": {},
   "source": [
    "## Export One Test Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74aa0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_one_test_graph():\n",
    "    PPI_PATH = \"./data/ppi/\"\n",
    "    node_features = np.load(os.path.join(PPI_PATH, 'test_feats.npy'))\n",
    "    node_labels = np.load(os.path.join(PPI_PATH, 'test_labels.npy'))\n",
    "    # Graph topology stored in a special nodes-links NetworkX format\n",
    "    nodes_links_dict = json_read(os.path.join(PPI_PATH, 'test_graph.json'))\n",
    "    # PPI contains undirected graphs with self edges - 20 train graphs, 2 validation graphs and 2 test graphs\n",
    "    # The reason I use a NetworkX's directed graph is because we need to explicitly model both directions\n",
    "    # because of the edge index and the way GAT implementation #3 works\n",
    "    collection_of_graphs = nx.DiGraph(json_graph.node_link_graph(nodes_links_dict))\n",
    "    # For each node in the above collection, ids specify to which graph the node belongs to\n",
    "    graph_ids = np.load(os.path.join(PPI_PATH, 'test_graph_id.npy'))\n",
    "    # Only load test graph 23\n",
    "    mask = graph_ids == 23  # find the nodes which belong to the current graph (identified via id)\n",
    "    graph_node_ids = np.asarray(mask).nonzero()[0]\n",
    "    graph = collection_of_graphs.subgraph(graph_node_ids)\n",
    "    return graph, node_features[mask], node_labels[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef70002",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph, features, labels = load_one_test_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38c32be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes =  3224  edges = 103872\n"
     ]
    }
   ],
   "source": [
    "print(\"number of nodes = \", len(test_graph.nodes), \" edges =\", len(test_graph.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfafed32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 163),\n",
       " (0, 165),\n",
       " (0, 569),\n",
       " (0, 633),\n",
       " (0, 634),\n",
       " (0, 685),\n",
       " (0, 702),\n",
       " (0, 793),\n",
       " (0, 796),\n",
       " (0, 1230),\n",
       " (0, 1804),\n",
       " (0, 1864),\n",
       " (0, 2350),\n",
       " (0, 2593),\n",
       " (0, 2746),\n",
       " (0, 2771),\n",
       " (0, 3000),\n",
       " (0, 3108),\n",
       " (0, 3149),\n",
       " (1, 1),\n",
       " (1, 16),\n",
       " (1, 67),\n",
       " (2, 2),\n",
       " (2, 8),\n",
       " (2, 48),\n",
       " (2, 201),\n",
       " (2, 442),\n",
       " (2, 489),\n",
       " (2, 547),\n",
       " (2, 672),\n",
       " (2, 684),\n",
       " (2, 685),\n",
       " (2, 808),\n",
       " (2, 826),\n",
       " (2, 1073),\n",
       " (2, 1561),\n",
       " (2, 1707),\n",
       " (2, 1883),\n",
       " (2, 2061),\n",
       " (2, 2218),\n",
       " (2, 2305),\n",
       " (2, 2308),\n",
       " (2, 2478),\n",
       " (2, 2714),\n",
       " (2, 2724),\n",
       " (2, 3134),\n",
       " (2, 3151),\n",
       " (3, 3),\n",
       " (3, 90),\n",
       " (3, 125),\n",
       " (3, 126),\n",
       " (3, 186),\n",
       " (3, 261),\n",
       " (3, 265),\n",
       " (3, 332),\n",
       " (3, 360),\n",
       " (3, 361),\n",
       " (3, 363),\n",
       " (3, 366),\n",
       " (3, 367),\n",
       " (3, 482),\n",
       " (3, 590),\n",
       " (3, 591),\n",
       " (3, 592),\n",
       " (3, 668),\n",
       " (3, 732),\n",
       " (3, 796),\n",
       " (3, 815),\n",
       " (3, 892),\n",
       " (3, 965),\n",
       " (3, 1064),\n",
       " (3, 1127),\n",
       " (3, 1130),\n",
       " (3, 1134),\n",
       " (3, 1267),\n",
       " (3, 1306),\n",
       " (3, 1311),\n",
       " (3, 1312),\n",
       " (3, 1313),\n",
       " (3, 1351),\n",
       " (3, 1441),\n",
       " (3, 1492),\n",
       " (3, 1529),\n",
       " (3, 1746),\n",
       " (3, 1788),\n",
       " (3, 1864),\n",
       " (3, 1883),\n",
       " (3, 1925),\n",
       " (3, 2006),\n",
       " (3, 2116),\n",
       " (3, 2160),\n",
       " (3, 2246),\n",
       " (3, 2265),\n",
       " (3, 2312),\n",
       " (3, 2323),\n",
       " (3, 2337),\n",
       " (3, 2491),\n",
       " (3, 2577),\n",
       " (3, 2601),\n",
       " (3, 2661),\n",
       " (3, 2703),\n",
       " (3, 2758),\n",
       " (3, 2790),\n",
       " (3, 3088),\n",
       " (4, 4),\n",
       " (4, 1404),\n",
       " (4, 1617),\n",
       " (5, 5),\n",
       " (5, 12),\n",
       " (5, 13),\n",
       " (5, 108),\n",
       " (5, 207),\n",
       " (5, 278),\n",
       " (5, 302),\n",
       " (5, 422),\n",
       " (5, 445),\n",
       " (5, 447),\n",
       " (5, 509),\n",
       " (5, 612),\n",
       " (5, 684),\n",
       " (5, 857),\n",
       " (5, 949),\n",
       " (5, 1024),\n",
       " (5, 1267),\n",
       " (5, 1310),\n",
       " (5, 1311),\n",
       " (5, 1312),\n",
       " (5, 1332),\n",
       " (5, 1345),\n",
       " (5, 1482),\n",
       " (5, 1497),\n",
       " (5, 1665),\n",
       " (5, 1709),\n",
       " (5, 1774),\n",
       " (5, 1779),\n",
       " (5, 1809),\n",
       " (5, 1907),\n",
       " (5, 2036),\n",
       " (5, 2136),\n",
       " (5, 2166),\n",
       " (5, 2184),\n",
       " (5, 2229),\n",
       " (5, 2403),\n",
       " (5, 2409),\n",
       " (5, 2448),\n",
       " (5, 2513),\n",
       " (5, 2514),\n",
       " (5, 2686),\n",
       " (5, 2898),\n",
       " (5, 2914),\n",
       " (5, 2963),\n",
       " (5, 2991),\n",
       " (5, 3138),\n",
       " (5, 3142),\n",
       " (5, 3156),\n",
       " (6, 6),\n",
       " (6, 36),\n",
       " (6, 131),\n",
       " (6, 274),\n",
       " (6, 704),\n",
       " (6, 896),\n",
       " (6, 1008),\n",
       " (6, 1345),\n",
       " (6, 2450),\n",
       " (6, 2836),\n",
       " (6, 3000),\n",
       " (7, 7),\n",
       " (7, 488),\n",
       " (7, 605),\n",
       " (7, 961),\n",
       " (7, 1075),\n",
       " (7, 1395),\n",
       " (7, 1418),\n",
       " (7, 1998),\n",
       " (7, 2136),\n",
       " (7, 2969),\n",
       " (7, 3170),\n",
       " (8, 2),\n",
       " (8, 8),\n",
       " (8, 52),\n",
       " (8, 93),\n",
       " (8, 131),\n",
       " (8, 177),\n",
       " (8, 178),\n",
       " (8, 185),\n",
       " (8, 190),\n",
       " (8, 202),\n",
       " (8, 237),\n",
       " (8, 336),\n",
       " (8, 421),\n",
       " (8, 422),\n",
       " (8, 472),\n",
       " (8, 482),\n",
       " (8, 504),\n",
       " (8, 595),\n",
       " (8, 602),\n",
       " (8, 604),\n",
       " (8, 608),\n",
       " (8, 609),\n",
       " (8, 611),\n",
       " (8, 613),\n",
       " (8, 625),\n",
       " (8, 632),\n",
       " (8, 660),\n",
       " (8, 664),\n",
       " (8, 714),\n",
       " (8, 715),\n",
       " (8, 753),\n",
       " (8, 852),\n",
       " (8, 955),\n",
       " (8, 1026),\n",
       " (8, 1032),\n",
       " (8, 1099),\n",
       " (8, 1143),\n",
       " (8, 1251),\n",
       " (8, 1252),\n",
       " (8, 1279),\n",
       " (8, 1330),\n",
       " (8, 1343),\n",
       " (8, 1398),\n",
       " (8, 1416),\n",
       " (8, 1417),\n",
       " (8, 1418),\n",
       " (8, 1445),\n",
       " (8, 1539),\n",
       " (8, 1554),\n",
       " (8, 1657),\n",
       " (8, 1902),\n",
       " (8, 1914),\n",
       " (8, 1918),\n",
       " (8, 1936),\n",
       " (8, 1945),\n",
       " (8, 1990),\n",
       " (8, 1991),\n",
       " (8, 2024),\n",
       " (8, 2025),\n",
       " (8, 2071),\n",
       " (8, 2122),\n",
       " (8, 2319),\n",
       " (8, 2428),\n",
       " (8, 2500),\n",
       " (8, 2667),\n",
       " (8, 2740),\n",
       " (8, 2751),\n",
       " (8, 2792),\n",
       " (8, 2810),\n",
       " (8, 2817),\n",
       " (8, 2821),\n",
       " (8, 2831),\n",
       " (8, 2937),\n",
       " (8, 3023),\n",
       " (8, 3062),\n",
       " (8, 3111),\n",
       " (8, 3211),\n",
       " (9, 9),\n",
       " (9, 108),\n",
       " (9, 209),\n",
       " (9, 236),\n",
       " (9, 409),\n",
       " (9, 410),\n",
       " (9, 1025),\n",
       " (9, 2122),\n",
       " (10, 10),\n",
       " (10, 3000),\n",
       " (11, 11),\n",
       " (11, 2312),\n",
       " (11, 3000),\n",
       " (12, 5),\n",
       " (12, 12),\n",
       " (12, 14),\n",
       " (12, 89),\n",
       " (12, 96),\n",
       " (12, 107),\n",
       " (12, 207),\n",
       " (12, 332),\n",
       " (12, 379),\n",
       " (12, 401),\n",
       " (12, 409),\n",
       " (12, 505),\n",
       " (12, 506),\n",
       " (12, 541),\n",
       " (12, 565),\n",
       " (12, 609),\n",
       " (12, 612),\n",
       " (12, 629),\n",
       " (12, 669),\n",
       " (12, 684),\n",
       " (12, 719),\n",
       " (12, 725),\n",
       " (12, 732),\n",
       " (12, 807),\n",
       " (12, 820),\n",
       " (12, 845),\n",
       " (12, 853),\n",
       " (12, 854),\n",
       " (12, 857),\n",
       " (12, 867),\n",
       " (12, 915),\n",
       " (12, 939),\n",
       " (12, 1081),\n",
       " (12, 1086),\n",
       " (12, 1141),\n",
       " (12, 1294),\n",
       " (12, 1324),\n",
       " (12, 1350),\n",
       " (12, 1388),\n",
       " (12, 1485),\n",
       " (12, 1527),\n",
       " (12, 1563),\n",
       " (12, 1644),\n",
       " (12, 1668),\n",
       " (12, 1686),\n",
       " (12, 1699),\n",
       " (12, 1702),\n",
       " (12, 1819),\n",
       " (12, 1834),\n",
       " (12, 1868),\n",
       " (12, 1873),\n",
       " (12, 1891),\n",
       " (12, 1892),\n",
       " (12, 1993),\n",
       " (12, 2009),\n",
       " (12, 2026),\n",
       " (12, 2047),\n",
       " (12, 2087),\n",
       " (12, 2093),\n",
       " (12, 2148),\n",
       " (12, 2368),\n",
       " (12, 2372),\n",
       " (12, 2392),\n",
       " (12, 2403),\n",
       " (12, 2421),\n",
       " (12, 2437),\n",
       " (12, 2475),\n",
       " (12, 2560),\n",
       " (12, 2671),\n",
       " (12, 2711),\n",
       " (12, 2740),\n",
       " (12, 2797),\n",
       " (12, 2801),\n",
       " (12, 2806),\n",
       " (12, 2880),\n",
       " (12, 2893),\n",
       " (12, 2931),\n",
       " (12, 2993),\n",
       " (12, 3039),\n",
       " (12, 3048),\n",
       " (13, 5),\n",
       " (13, 13),\n",
       " (13, 16),\n",
       " (13, 48),\n",
       " (13, 108),\n",
       " (13, 209),\n",
       " (13, 308),\n",
       " (13, 373),\n",
       " (13, 468),\n",
       " (13, 527),\n",
       " (13, 803),\n",
       " (13, 857),\n",
       " (13, 946),\n",
       " (13, 1158),\n",
       " (13, 1312),\n",
       " (13, 1561),\n",
       " (13, 1829),\n",
       " (13, 1831),\n",
       " (13, 2421),\n",
       " (13, 2483),\n",
       " (13, 2512),\n",
       " (13, 2621),\n",
       " (13, 2870),\n",
       " (13, 2993),\n",
       " (13, 3151),\n",
       " (13, 3202),\n",
       " (13, 3207),\n",
       " (14, 12),\n",
       " (14, 14),\n",
       " (14, 89),\n",
       " (14, 159),\n",
       " (14, 447),\n",
       " (14, 473),\n",
       " (14, 505),\n",
       " (14, 578),\n",
       " (14, 607),\n",
       " (14, 662),\n",
       " (14, 691),\n",
       " (14, 799),\n",
       " (14, 807),\n",
       " (14, 818),\n",
       " (14, 820),\n",
       " (14, 888),\n",
       " (14, 1004),\n",
       " (14, 1065),\n",
       " (14, 1172),\n",
       " (14, 1267),\n",
       " (14, 1349),\n",
       " (14, 1395),\n",
       " (14, 1493),\n",
       " (14, 1602),\n",
       " (14, 1668),\n",
       " (14, 1883),\n",
       " (14, 1947),\n",
       " (14, 1994),\n",
       " (14, 2034),\n",
       " (14, 2064),\n",
       " (14, 2136),\n",
       " (14, 2184),\n",
       " (14, 2328),\n",
       " (14, 2349),\n",
       " (14, 2560),\n",
       " (14, 2643),\n",
       " (14, 2669),\n",
       " (14, 2677),\n",
       " (14, 2808),\n",
       " (14, 2811),\n",
       " (14, 2880),\n",
       " (14, 2901),\n",
       " (14, 2993),\n",
       " (14, 3038),\n",
       " (14, 3193),\n",
       " (15, 15),\n",
       " (15, 1035),\n",
       " (15, 1248),\n",
       " (15, 1249),\n",
       " (15, 2486),\n",
       " (15, 2808),\n",
       " (15, 2811),\n",
       " (16, 1),\n",
       " (16, 13),\n",
       " (16, 16),\n",
       " (16, 41),\n",
       " (16, 65),\n",
       " (16, 73),\n",
       " (16, 86),\n",
       " (16, 91),\n",
       " (16, 96),\n",
       " (16, 104),\n",
       " (16, 107),\n",
       " (16, 111),\n",
       " (16, 112),\n",
       " (16, 114),\n",
       " (16, 127),\n",
       " (16, 135),\n",
       " (16, 145),\n",
       " (16, 154),\n",
       " (16, 157),\n",
       " (16, 163),\n",
       " (16, 165),\n",
       " (16, 166),\n",
       " (16, 179),\n",
       " (16, 182),\n",
       " (16, 201),\n",
       " (16, 202),\n",
       " (16, 205),\n",
       " (16, 216),\n",
       " (16, 232),\n",
       " (16, 242),\n",
       " (16, 244),\n",
       " (16, 249),\n",
       " (16, 250),\n",
       " (16, 257),\n",
       " (16, 270),\n",
       " (16, 282),\n",
       " (16, 289),\n",
       " (16, 296),\n",
       " (16, 307),\n",
       " (16, 322),\n",
       " (16, 327),\n",
       " (16, 330),\n",
       " (16, 340),\n",
       " (16, 343),\n",
       " (16, 352),\n",
       " (16, 369),\n",
       " (16, 371),\n",
       " (16, 377),\n",
       " (16, 379),\n",
       " (16, 381),\n",
       " (16, 401),\n",
       " (16, 413),\n",
       " (16, 422),\n",
       " (16, 423),\n",
       " (16, 434),\n",
       " (16, 448),\n",
       " (16, 449),\n",
       " (16, 454),\n",
       " (16, 465),\n",
       " (16, 466),\n",
       " (16, 467),\n",
       " (16, 469),\n",
       " (16, 470),\n",
       " (16, 472),\n",
       " (16, 473),\n",
       " (16, 485),\n",
       " (16, 492),\n",
       " (16, 493),\n",
       " (16, 505),\n",
       " (16, 508),\n",
       " (16, 542),\n",
       " (16, 543),\n",
       " (16, 546),\n",
       " (16, 570),\n",
       " (16, 571),\n",
       " (16, 574),\n",
       " (16, 578),\n",
       " (16, 579),\n",
       " (16, 580),\n",
       " (16, 582),\n",
       " (16, 583),\n",
       " (16, 596),\n",
       " (16, 609),\n",
       " (16, 611),\n",
       " (16, 624),\n",
       " (16, 643),\n",
       " (16, 645),\n",
       " (16, 647),\n",
       " (16, 651),\n",
       " (16, 654),\n",
       " (16, 664),\n",
       " (16, 666),\n",
       " (16, 690),\n",
       " (16, 701),\n",
       " (16, 716),\n",
       " (16, 720),\n",
       " (16, 725),\n",
       " (16, 727),\n",
       " (16, 742),\n",
       " (16, 748),\n",
       " (16, 750),\n",
       " (16, 755),\n",
       " (16, 756),\n",
       " (16, 759),\n",
       " (16, 760),\n",
       " (16, 768),\n",
       " (16, 775),\n",
       " (16, 784),\n",
       " (16, 811),\n",
       " (16, 821),\n",
       " (16, 824),\n",
       " (16, 825),\n",
       " (16, 836),\n",
       " (16, 838),\n",
       " (16, 842),\n",
       " (16, 849),\n",
       " (16, 852),\n",
       " (16, 854),\n",
       " (16, 857),\n",
       " (16, 873),\n",
       " (16, 887),\n",
       " (16, 888),\n",
       " (16, 892),\n",
       " (16, 893),\n",
       " (16, 895),\n",
       " (16, 904),\n",
       " (16, 907),\n",
       " (16, 908),\n",
       " (16, 916),\n",
       " (16, 959),\n",
       " (16, 960),\n",
       " (16, 966),\n",
       " (16, 986),\n",
       " (16, 988),\n",
       " (16, 999),\n",
       " (16, 1002),\n",
       " (16, 1004),\n",
       " (16, 1012),\n",
       " (16, 1017),\n",
       " (16, 1022),\n",
       " (16, 1025),\n",
       " (16, 1070),\n",
       " (16, 1078),\n",
       " (16, 1079),\n",
       " (16, 1080),\n",
       " (16, 1089),\n",
       " (16, 1099),\n",
       " (16, 1104),\n",
       " (16, 1108),\n",
       " (16, 1141),\n",
       " (16, 1146),\n",
       " (16, 1147),\n",
       " (16, 1149),\n",
       " (16, 1156),\n",
       " (16, 1173),\n",
       " (16, 1206),\n",
       " (16, 1222),\n",
       " (16, 1231),\n",
       " (16, 1237),\n",
       " (16, 1251),\n",
       " (16, 1260),\n",
       " (16, 1267),\n",
       " (16, 1268),\n",
       " (16, 1269),\n",
       " (16, 1303),\n",
       " (16, 1305),\n",
       " (16, 1306),\n",
       " (16, 1307),\n",
       " (16, 1308),\n",
       " (16, 1310),\n",
       " (16, 1313),\n",
       " (16, 1326),\n",
       " (16, 1345),\n",
       " (16, 1348),\n",
       " (16, 1349),\n",
       " (16, 1350),\n",
       " (16, 1351),\n",
       " (16, 1365),\n",
       " (16, 1376),\n",
       " (16, 1385),\n",
       " (16, 1388),\n",
       " (16, 1393),\n",
       " (16, 1402),\n",
       " (16, 1406),\n",
       " (16, 1416),\n",
       " (16, 1417),\n",
       " (16, 1418),\n",
       " (16, 1423),\n",
       " (16, 1441),\n",
       " (16, 1448),\n",
       " (16, 1449),\n",
       " (16, 1452),\n",
       " (16, 1456),\n",
       " (16, 1457),\n",
       " (16, 1464),\n",
       " (16, 1482),\n",
       " (16, 1491),\n",
       " (16, 1497),\n",
       " (16, 1506),\n",
       " (16, 1507),\n",
       " (16, 1509),\n",
       " (16, 1522),\n",
       " (16, 1523),\n",
       " (16, 1526),\n",
       " (16, 1532),\n",
       " (16, 1544),\n",
       " (16, 1549),\n",
       " (16, 1554),\n",
       " (16, 1561),\n",
       " (16, 1564),\n",
       " (16, 1565),\n",
       " (16, 1574),\n",
       " (16, 1590),\n",
       " (16, 1592),\n",
       " (16, 1604),\n",
       " (16, 1606),\n",
       " (16, 1613),\n",
       " (16, 1629),\n",
       " (16, 1633),\n",
       " (16, 1639),\n",
       " (16, 1644),\n",
       " (16, 1647),\n",
       " (16, 1663),\n",
       " (16, 1674),\n",
       " (16, 1732),\n",
       " (16, 1750),\n",
       " (16, 1753),\n",
       " (16, 1773),\n",
       " (16, 1787),\n",
       " (16, 1812),\n",
       " (16, 1822),\n",
       " (16, 1825),\n",
       " (16, 1834),\n",
       " (16, 1837),\n",
       " (16, 1847),\n",
       " (16, 1853),\n",
       " (16, 1863),\n",
       " (16, 1865),\n",
       " (16, 1872),\n",
       " (16, 1881),\n",
       " (16, 1883),\n",
       " (16, 1894),\n",
       " (16, 1898),\n",
       " (16, 1903),\n",
       " (16, 1906),\n",
       " (16, 1923),\n",
       " (16, 1953),\n",
       " (16, 1954),\n",
       " (16, 1955),\n",
       " (16, 1961),\n",
       " (16, 1963),\n",
       " (16, 1965),\n",
       " (16, 1970),\n",
       " (16, 1972),\n",
       " (16, 2021),\n",
       " (16, 2025),\n",
       " (16, 2027),\n",
       " (16, 2037),\n",
       " (16, 2041),\n",
       " (16, 2051),\n",
       " (16, 2053),\n",
       " (16, 2072),\n",
       " (16, 2073),\n",
       " (16, 2080),\n",
       " (16, 2097),\n",
       " (16, 2108),\n",
       " (16, 2112),\n",
       " (16, 2120),\n",
       " (16, 2122),\n",
       " (16, 2135),\n",
       " (16, 2145),\n",
       " (16, 2153),\n",
       " (16, 2166),\n",
       " (16, 2169),\n",
       " (16, 2173),\n",
       " (16, 2179),\n",
       " (16, 2184),\n",
       " (16, 2206),\n",
       " (16, 2207),\n",
       " (16, 2209),\n",
       " (16, 2214),\n",
       " (16, 2222),\n",
       " (16, 2225),\n",
       " (16, 2227),\n",
       " (16, 2230),\n",
       " (16, 2233),\n",
       " (16, 2268),\n",
       " (16, 2272),\n",
       " (16, 2279),\n",
       " (16, 2294),\n",
       " (16, 2295),\n",
       " (16, 2297),\n",
       " (16, 2310),\n",
       " (16, 2317),\n",
       " (16, 2337),\n",
       " (16, 2339),\n",
       " (16, 2342),\n",
       " (16, 2343),\n",
       " (16, 2350),\n",
       " (16, 2353),\n",
       " (16, 2356),\n",
       " (16, 2358),\n",
       " (16, 2371),\n",
       " (16, 2373),\n",
       " (16, 2378),\n",
       " (16, 2400),\n",
       " (16, 2402),\n",
       " (16, 2405),\n",
       " (16, 2407),\n",
       " (16, 2413),\n",
       " (16, 2423),\n",
       " (16, 2426),\n",
       " (16, 2427),\n",
       " (16, 2428),\n",
       " (16, 2429),\n",
       " (16, 2434),\n",
       " (16, 2435),\n",
       " (16, 2437),\n",
       " (16, 2447),\n",
       " (16, 2448),\n",
       " (16, 2460),\n",
       " (16, 2470),\n",
       " (16, 2483),\n",
       " (16, 2485),\n",
       " (16, 2493),\n",
       " (16, 2497),\n",
       " (16, 2499),\n",
       " (16, 2501),\n",
       " (16, 2504),\n",
       " (16, 2520),\n",
       " (16, 2537),\n",
       " (16, 2539),\n",
       " (16, 2555),\n",
       " (16, 2567),\n",
       " (16, 2578),\n",
       " (16, 2583),\n",
       " (16, 2592),\n",
       " (16, 2596),\n",
       " (16, 2601),\n",
       " (16, 2618),\n",
       " (16, 2621),\n",
       " (16, 2629),\n",
       " (16, 2644),\n",
       " (16, 2648),\n",
       " (16, 2658),\n",
       " (16, 2661),\n",
       " (16, 2669),\n",
       " (16, 2671),\n",
       " (16, 2684),\n",
       " (16, 2693),\n",
       " (16, 2704),\n",
       " (16, 2709),\n",
       " (16, 2711),\n",
       " (16, 2715),\n",
       " (16, 2722),\n",
       " (16, 2725),\n",
       " (16, 2739),\n",
       " (16, 2750),\n",
       " (16, 2751),\n",
       " (16, 2755),\n",
       " (16, 2759),\n",
       " (16, 2760),\n",
       " (16, 2769),\n",
       " (16, 2781),\n",
       " (16, 2788),\n",
       " (16, 2790),\n",
       " (16, 2797),\n",
       " (16, 2798),\n",
       " (16, 2805),\n",
       " (16, 2807),\n",
       " (16, 2831),\n",
       " (16, 2835),\n",
       " (16, 2842),\n",
       " (16, 2844),\n",
       " (16, 2851),\n",
       " (16, 2855),\n",
       " (16, 2861),\n",
       " (16, 2879),\n",
       " (16, 2880),\n",
       " (16, 2892),\n",
       " (16, 2895),\n",
       " (16, 2900),\n",
       " (16, 2906),\n",
       " (16, 2928),\n",
       " (16, 2933),\n",
       " (16, 2934),\n",
       " (16, 2936),\n",
       " (16, 2940),\n",
       " (16, 2954),\n",
       " (16, 2961),\n",
       " (16, 2963),\n",
       " (16, 2970),\n",
       " (16, 2993),\n",
       " (16, 2997),\n",
       " (16, 3017),\n",
       " (16, 3023),\n",
       " (16, 3035),\n",
       " (16, 3047),\n",
       " (16, 3048),\n",
       " (16, 3049),\n",
       " (16, 3060),\n",
       " (16, 3066),\n",
       " (16, 3070),\n",
       " (16, 3087),\n",
       " (16, 3099),\n",
       " (16, 3112),\n",
       " (16, 3116),\n",
       " (16, 3124),\n",
       " (16, 3134),\n",
       " (16, 3139),\n",
       " (16, 3143),\n",
       " (16, 3151),\n",
       " (16, 3165),\n",
       " (16, 3174),\n",
       " (16, 3190),\n",
       " (16, 3198),\n",
       " (16, 3208),\n",
       " (17, 17),\n",
       " (17, 2486),\n",
       " (17, 2489),\n",
       " (18, 18),\n",
       " (18, 42),\n",
       " (18, 100),\n",
       " (18, 112),\n",
       " (18, 332),\n",
       " (18, 342),\n",
       " (18, 369),\n",
       " (18, 542),\n",
       " (18, 555),\n",
       " (18, 611),\n",
       " (18, 613),\n",
       " (18, 725),\n",
       " (18, 759),\n",
       " (18, 760),\n",
       " (18, 799),\n",
       " (18, 853),\n",
       " (18, 854),\n",
       " (18, 857),\n",
       " (18, 916),\n",
       " (18, 1098),\n",
       " (18, 1099),\n",
       " (18, 1156),\n",
       " (18, 1269),\n",
       " (18, 1351),\n",
       " (18, 1452),\n",
       " (18, 1477),\n",
       " (18, 1707),\n",
       " (18, 1916),\n",
       " (18, 2034),\n",
       " (18, 2072),\n",
       " (18, 2265),\n",
       " (18, 2272),\n",
       " (18, 2279),\n",
       " (18, 2405),\n",
       " (18, 2429),\n",
       " (18, 2447),\n",
       " (18, 2448),\n",
       " (18, 2601),\n",
       " (18, 2648),\n",
       " (18, 2659),\n",
       " (18, 3190),\n",
       " (18, 3206),\n",
       " (18, 3207),\n",
       " (19, 19),\n",
       " (19, 117),\n",
       " (19, 209),\n",
       " (19, 275),\n",
       " (19, 685),\n",
       " (19, 824),\n",
       " (19, 1012),\n",
       " (19, 1254),\n",
       " (19, 1710),\n",
       " (19, 1773),\n",
       " (19, 1991),\n",
       " (19, 2207),\n",
       " (19, 2601),\n",
       " (19, 2850),\n",
       " (20, 20),\n",
       " (20, 48),\n",
       " (20, 685),\n",
       " (20, 1016),\n",
       " (20, 1848),\n",
       " (20, 2162),\n",
       " (20, 2478),\n",
       " (20, 2680),\n",
       " (20, 3134),\n",
       " (21, 21),\n",
       " (21, 172),\n",
       " (21, 198),\n",
       " (21, 223),\n",
       " (21, 700),\n",
       " (21, 793),\n",
       " (21, 824),\n",
       " (21, 977),\n",
       " (21, 1014),\n",
       " (21, 1049),\n",
       " (21, 1081),\n",
       " (21, 1293),\n",
       " (21, 1306),\n",
       " (21, 1361),\n",
       " (21, 1562),\n",
       " (21, 1643),\n",
       " (21, 1728),\n",
       " (21, 1731),\n",
       " (21, 1791),\n",
       " (21, 1807),\n",
       " (21, 1829),\n",
       " (21, 1945),\n",
       " (21, 1949),\n",
       " (21, 2001),\n",
       " (21, 2117),\n",
       " (21, 2142),\n",
       " (21, 2503),\n",
       " (21, 2571),\n",
       " (21, 2598),\n",
       " (21, 2937),\n",
       " (21, 3000),\n",
       " (22, 22),\n",
       " (22, 881),\n",
       " (22, 1034),\n",
       " (22, 1059),\n",
       " (22, 2353),\n",
       " (22, 2592),\n",
       " (23, 23),\n",
       " (23, 231),\n",
       " (23, 269),\n",
       " (23, 604),\n",
       " (23, 927),\n",
       " (23, 1306),\n",
       " (23, 1349),\n",
       " (23, 1527),\n",
       " (23, 1565),\n",
       " (23, 1624),\n",
       " (23, 1962),\n",
       " (23, 1990),\n",
       " (23, 2025),\n",
       " (23, 2102),\n",
       " (23, 2128),\n",
       " (23, 2616),\n",
       " (23, 2651),\n",
       " (23, 2661),\n",
       " (23, 2725),\n",
       " (23, 2830),\n",
       " (23, 2907),\n",
       " (23, 3006),\n",
       " (23, 3056),\n",
       " (24, 24),\n",
       " (24, 360),\n",
       " (24, 363),\n",
       " (24, 367),\n",
       " (24, 592),\n",
       " (24, 879),\n",
       " (24, 895),\n",
       " (24, 1061),\n",
       " (24, 1064),\n",
       " (24, 1746),\n",
       " (24, 1936),\n",
       " (24, 2021),\n",
       " (24, 2155),\n",
       " (24, 2169),\n",
       " (24, 2604),\n",
       " (24, 2658),\n",
       " (24, 2775),\n",
       " (24, 3056),\n",
       " (25, 25),\n",
       " (25, 61),\n",
       " (25, 1268),\n",
       " (25, 1312),\n",
       " (25, 1866),\n",
       " (25, 2486),\n",
       " (25, 2730),\n",
       " (26, 26),\n",
       " (26, 1086),\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(test_graph.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18d7bb",
   "metadata": {},
   "source": [
    "### Export Graph Topology (Sparse Matrix Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a14172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "adj_file = open(\"./ppi_adj.txt\", \"w\")\n",
    "# first line: number of nodes, number of edges\n",
    "adj_file.write(str(test_graph.number_of_nodes()) + \" \" + str(test_graph.number_of_edges()) + \"\\n\")\n",
    "# second line: column indices of each non-zero element\n",
    "sorted_edges = sorted(test_graph.edges)\n",
    "for edge in sorted_edges:\n",
    "    adj_file.write(str(edge[1]) + \" \")\n",
    "adj_file.write(\"\\n\")\n",
    "# third line: number of non-zero elements in last row\n",
    "adj_file.write(\"0\")\n",
    "edge_counter = defaultdict(lambda: 0)\n",
    "for edge in sorted_edges:\n",
    "    edge_counter[edge[0]] += 1\n",
    "counter = 0\n",
    "for i in range(test_graph.number_of_nodes()):\n",
    "    counter += edge_counter[i]\n",
    "    adj_file.write(\" \" + str(counter))\n",
    "adj_file.write(\"\\n\")\n",
    "adj_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deca855",
   "metadata": {},
   "source": [
    "### Export Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_file = open(\"./ppi_features.txt\", \"w\")\n",
    "for feature in features:\n",
    "    for feat in feature:\n",
    "        feature_file.write(str(feat) + \" \")\n",
    "    feature_file.write(\"\\n\")\n",
    "feature_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c7fed",
   "metadata": {},
   "source": [
    "### Export Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file = open(\"./ppi_labels.txt\", \"w\")\n",
    "# 3224 lines, each line is a 121 dimension vector\n",
    "for label in labels:\n",
    "    for elem in label:\n",
    "        label_file.write(str(elem) + \" \")\n",
    "    label_file.write(\"\\n\")\n",
    "label_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
